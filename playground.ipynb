{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25adeb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cba216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadConvNNAttentionEnhanced(nn.Module):\n",
    "    \"\"\"Enhanced version with head-specific sampling and coordinate encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, d_hidden, num_heads, attention_dropout, K, sampling_type, \n",
    "                 num_samples, sample_padding, magnitude_type, seq_length=197, \n",
    "                 coordinate_encoding=False, diverse_sampling=True):\n",
    "        super(MultiHeadConvNNAttentionEnhanced, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.K = K\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples if num_samples != -1 else 'all'\n",
    "        self.sample_padding = sample_padding if sampling_type == 'spatial' else 0    \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'similarity' else False\n",
    "        self.diverse_sampling = diverse_sampling  # Enable head-specific sampling\n",
    "        \n",
    "        # Coordinate Encoding\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else d_hidden // num_heads\n",
    "        self.out_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else d_hidden // num_heads\n",
    "        self.kernel_size = K\n",
    "        self.stride = K\n",
    "        \n",
    "        # Shared Conv across heads\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=0,\n",
    "        )\n",
    "        \n",
    "        # Pointwise conv for coordinate removal\n",
    "        if coordinate_encoding:\n",
    "            self.pointwise_conv = nn.Conv1d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels - 1,\n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        # Optional: Head mixing layer for cross-head interaction\n",
    "        self.head_mixing = nn.Conv1d(d_hidden, d_hidden, 1, groups=1)\n",
    "        self.mix_weight = nn.Parameter(torch.tensor([0.1]))  # Learnable mixing weight\n",
    "        \n",
    "    def get_head_specific_samples(self, seq_len, num_heads, device):\n",
    "        \"\"\"Generate different samples for each head\"\"\"\n",
    "        if self.sampling_type == 'random':\n",
    "            # Different random samples for each head\n",
    "            all_indices = []\n",
    "            for h in range(num_heads):\n",
    "                # Add seed offset per head for reproducibility if needed\n",
    "                rand_idx = torch.randperm(seq_len, device=device)[:self.num_samples]\n",
    "                all_indices.append(rand_idx)\n",
    "            return torch.stack(all_indices)  # (H, num_samples)\n",
    "            \n",
    "        elif self.sampling_type == 'spatial':\n",
    "            # Different spatial patterns per head\n",
    "            all_indices = []\n",
    "            for h in range(num_heads):\n",
    "                # Vary the spatial sampling pattern per head\n",
    "                offset = h * (seq_len // (num_heads * 2))  # Stagger starting points\n",
    "                start = (offset + self.sample_padding) % seq_len\n",
    "                end = seq_len - self.sample_padding - 1\n",
    "                \n",
    "                # Ensure we don't go out of bounds\n",
    "                if start >= end:\n",
    "                    start = self.sample_padding\n",
    "                    \n",
    "                spat_idx = torch.linspace(start, end, self.num_samples, device=device).long()\n",
    "                spat_idx = torch.clamp(spat_idx, 0, seq_len - 1)  # Safety clamp\n",
    "                all_indices.append(spat_idx)\n",
    "            return torch.stack(all_indices)  # (H, num_samples)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid sampling_type: {self.sampling_type}\")\n",
    "    \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "    \n",
    "    def _add_coordinate_encoding_multihead(self, x, head_offset=0):\n",
    "        \"\"\"Add coordinate encoding with head-specific patterns\"\"\"\n",
    "        b, c, t = x.shape\n",
    "        \n",
    "        # Create head-specific coordinate patterns\n",
    "        num_heads_in_batch = b // self.batch_size\n",
    "        \n",
    "        coords_list = []\n",
    "        for i in range(b):\n",
    "            head_idx = i % num_heads_in_batch\n",
    "            \n",
    "            # Vary coordinate encoding per head\n",
    "            scale = 1.0 + 0.2 * (head_idx / num_heads_in_batch)  # Varying scales\n",
    "            phase = 2 * torch.pi * head_idx / num_heads_in_batch  # Phase shift\n",
    "            \n",
    "            # Linear coordinates with head-specific transformation\n",
    "            coords = torch.linspace(-scale, scale, t, device=x.device)\n",
    "            \n",
    "            # Optional: Add sinusoidal variation per head\n",
    "            coords = coords + 0.1 * torch.sin(coords * torch.pi + phase)\n",
    "            \n",
    "            coords_list.append(coords)\n",
    "        \n",
    "        coords_tensor = torch.stack(coords_list).unsqueeze(1)  # (b, 1, t)\n",
    "        x_with_coords = torch.cat((x, coords_tensor), dim=1)\n",
    "        return x_with_coords\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear projections\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        \n",
    "        # Process based on sampling type\n",
    "        if self.sampling_type == 'all':\n",
    "            # All samples - standard processing\n",
    "            q = self.batch_combine(self.split_head(q))\n",
    "            k = self.batch_combine(self.split_head(k))\n",
    "            v = self.batch_combine(self.split_head(v))\n",
    "            \n",
    "            # Add coordinate encoding\n",
    "            if self.coordinate_encoding:\n",
    "                q = self._add_coordinate_encoding_multihead(q)\n",
    "                k = self._add_coordinate_encoding_multihead(k)\n",
    "                v = self._add_coordinate_encoding_multihead(v)\n",
    "            \n",
    "            # ConvNN Algorithm\n",
    "            if self.magnitude_type == 'distance':\n",
    "                matrix_magnitude = self._calculate_distance_matrix(k, q, sqrt=True)\n",
    "            else:\n",
    "                matrix_magnitude = self._calculate_similarity_matrix(k, q)\n",
    "                \n",
    "            prime = self._prime(v, matrix_magnitude, self.K, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type in ['random', 'spatial']:\n",
    "            # Head-specific sampling\n",
    "            seq_len = x.shape[1]\n",
    "            \n",
    "            if self.diverse_sampling:\n",
    "                # Get different samples for each head\n",
    "                sample_indices = self.get_head_specific_samples(\n",
    "                    seq_len, self.num_heads, x.device\n",
    "                )  # (H, num_samples)\n",
    "            else:\n",
    "                # Use same samples for all heads (original behavior)\n",
    "                if self.sampling_type == 'random':\n",
    "                    base_idx = torch.randperm(seq_len, device=x.device)[:self.num_samples]\n",
    "                else:  # spatial\n",
    "                    base_idx = torch.linspace(\n",
    "                        self.sample_padding, \n",
    "                        seq_len - self.sample_padding - 1, \n",
    "                        self.num_samples, \n",
    "                        device=x.device\n",
    "                    ).long()\n",
    "                sample_indices = base_idx.unsqueeze(0).repeat(self.num_heads, 1)\n",
    "            \n",
    "            # Split heads first\n",
    "            q_heads = self.split_head(q)  # (B, H, L, D/H)\n",
    "            k_heads = self.split_head(k)  # (B, H, L, D/H)\n",
    "            v_heads = self.split_head(v)  # (B, H, L, D/H)\n",
    "            \n",
    "            # Process each head with its specific samples\n",
    "            prime_list = []\n",
    "            for h in range(self.num_heads):\n",
    "                idx = sample_indices[h]\n",
    "                \n",
    "                # Get head data\n",
    "                q_h = q_heads[:, h, idx, :].transpose(1, 2)  # (B, D/H, num_samples)\n",
    "                k_h = k_heads[:, h, :, :].transpose(1, 2)    # (B, D/H, L)\n",
    "                v_h = v_heads[:, h, :, :].transpose(1, 2)    # (B, D/H, L)\n",
    "                \n",
    "                # Add coordinate encoding if needed\n",
    "                if self.coordinate_encoding:\n",
    "                    q_h = self._add_coordinate_encoding_multihead(q_h, head_offset=h)\n",
    "                    k_h = self._add_coordinate_encoding_multihead(k_h, head_offset=h)\n",
    "                    v_h = self._add_coordinate_encoding_multihead(v_h, head_offset=h)\n",
    "                \n",
    "                # Calculate magnitude matrix\n",
    "                if self.magnitude_type == 'distance':\n",
    "                    matrix_magnitude = self._calculate_distance_matrix_N(k_h, q_h, sqrt=True)\n",
    "                else:\n",
    "                    matrix_magnitude = self._calculate_similarity_matrix_N(k_h, q_h)\n",
    "                \n",
    "                # Set diagonal to inf/-inf\n",
    "                range_idx = torch.arange(len(idx), device=x.device)\n",
    "                matrix_magnitude[:, idx, range_idx] = float('inf') if self.magnitude_type == 'distance' else float('-inf')\n",
    "                \n",
    "                # Get prime\n",
    "                prime_h = self._prime_N(v_h, matrix_magnitude, self.K, idx, self.maximum)\n",
    "                prime_list.append(prime_h)\n",
    "            \n",
    "            # Stack all head primes\n",
    "            prime = torch.cat(prime_list, dim=0)  # (B*H, D/H, L*K)\n",
    "        \n",
    "        # Apply convolution\n",
    "        x = self.conv(prime)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Remove coordinate channel if needed\n",
    "        if self.coordinate_encoding:\n",
    "            x = self.pointwise_conv(x)\n",
    "        \n",
    "        # Reshape back to (B, L, D)\n",
    "        x = self.combine_heads(self.batch_split(x.permute(0, 2, 1)))\n",
    "        \n",
    "        # Optional: Mix information across heads\n",
    "        x_mixed = self.head_mixing(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = x + self.mix_weight * x_mixed\n",
    "        \n",
    "        # Final output projection\n",
    "        x = self.W_o(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Keep all the original calculation methods\n",
    "    def _calculate_similarity_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(k_norm.transpose(2, 1), q_norm) \n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=0)  \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix_N(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(k_norm.transpose(2, 1), q_norm) \n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=0) \n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_distance_matrix(self, K, Q, sqrt=False):\n",
    "        norm_squared_K = torch.sum(K**2, dim=1, keepdim=True) \n",
    "        norm_squared_Q = torch.sum(Q**2, dim=1, keepdim=True) \n",
    "        dot_product = torch.bmm(K.transpose(2, 1), Q)  \n",
    "        dist_matrix = norm_squared_K + norm_squared_Q.transpose(2, 1) - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix\n",
    "\n",
    "    def _calculate_distance_matrix_N(self, K, Q, sqrt=False):\n",
    "        norm_squared_K = torch.sum(K**2, dim=1, keepdim=True).permute(0, 2, 1)\n",
    "        norm_squared_Q = torch.sum(Q**2, dim=1, keepdim=True).transpose(2, 1).permute(0, 2, 1)\n",
    "        dot_product = torch.bmm(K.transpose(2, 1), Q)  \n",
    "        dist_matrix = norm_squared_K + norm_squared_Q - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape \n",
    "        _, topk_indices = torch.topk(qk, k=K, dim=-1, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K)\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = prime.reshape(b, c, -1)\n",
    "        return prime\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        _, topk_indices = torch.topk(qk, k=K - 1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=indices_expanded)\n",
    "        prime = prime.reshape(b, c, -1)\n",
    "        return prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22462fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadConvNNAttentionSeparate(nn.Module):\n",
    "    \"\"\"Each head has its own ConvNN parameters\"\"\"\n",
    "    \n",
    "    def __init__(self, d_hidden, num_heads, attention_dropout, K, sampling_type, \n",
    "                 num_samples, sample_padding, magnitude_type, seq_length=197, \n",
    "                 coordinate_encoding=False):\n",
    "        super(MultiHeadConvNNAttentionSeparate, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.K = K\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples if num_samples != -1 else 'all'\n",
    "        self.sample_padding = sample_padding if sampling_type == 'spatial' else 0    \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'similarity' else False\n",
    "        \n",
    "        # Coordinate Encoding\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        self.in_channels = self.d_k + 1 if coordinate_encoding else self.d_k\n",
    "        self.out_channels = self.d_k + 1 if coordinate_encoding else self.d_k\n",
    "        \n",
    "        # Separate ConvNN for each head\n",
    "        self.conv_heads = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=K,\n",
    "                stride=K,\n",
    "                padding=0\n",
    "            ) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Separate pointwise conv for each head (if using coordinate encoding)\n",
    "        if coordinate_encoding:\n",
    "            self.pointwise_heads = nn.ModuleList([\n",
    "                nn.Conv1d(\n",
    "                    in_channels=self.out_channels,\n",
    "                    out_channels=self.d_k,\n",
    "                    kernel_size=1\n",
    "                ) for _ in range(num_heads)\n",
    "            ])\n",
    "        \n",
    "        # Optional: Learnable head-specific sampling offsets\n",
    "        if sampling_type in ['random', 'spatial']:\n",
    "            self.sampling_offsets = nn.Parameter(torch.randn(num_heads))\n",
    "    \n",
    "    def split_head(self, x):\n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden)\n",
    "    \n",
    "    def _add_coordinate_encoding_per_head(self, x, head_idx):\n",
    "        \"\"\"Add unique coordinate encoding per head\"\"\"\n",
    "        b, c, t = x.shape\n",
    "        \n",
    "        # Head-specific coordinate pattern\n",
    "        scale = 1.0 + 0.3 * torch.sigmoid(self.sampling_offsets[head_idx]).item() if hasattr(self, 'sampling_offsets') else 1.0\n",
    "        \n",
    "        # Create coordinates with head-specific transformations\n",
    "        coords = torch.linspace(-scale, scale, t, device=x.device)\n",
    "        \n",
    "        # Add non-linear transformation based on head index\n",
    "        if head_idx % 2 == 0:\n",
    "            # Even heads: use standard linear\n",
    "            coords = coords\n",
    "        else:\n",
    "            # Odd heads: use non-linear transformation\n",
    "            coords = torch.tanh(coords * (1 + head_idx * 0.1))\n",
    "        \n",
    "        coords = coords.unsqueeze(0).unsqueeze(0).expand(b, 1, -1)\n",
    "        x_with_coords = torch.cat((x, coords), dim=1)\n",
    "        return x_with_coords\n",
    "    \n",
    "    def process_head(self, q_h, k_h, v_h, head_idx):\n",
    "        \"\"\"Process a single head with its own ConvNN\"\"\"\n",
    "        \n",
    "        # Add coordinate encoding if needed\n",
    "        if self.coordinate_encoding:\n",
    "            q_h = self._add_coordinate_encoding_per_head(q_h, head_idx)\n",
    "            k_h = self._add_coordinate_encoding_per_head(k_h, head_idx)\n",
    "            v_h = self._add_coordinate_encoding_per_head(v_h, head_idx)\n",
    "        \n",
    "        if self.sampling_type == 'all':\n",
    "            # Calculate magnitude matrix\n",
    "            if self.magnitude_type == 'distance':\n",
    "                matrix_magnitude = self._calculate_distance_matrix(k_h, q_h, sqrt=True)\n",
    "            else:\n",
    "                matrix_magnitude = self._calculate_similarity_matrix(k_h, q_h)\n",
    "            \n",
    "            prime = self._prime(v_h, matrix_magnitude, self.K, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type in ['random', 'spatial']:\n",
    "            seq_len = k_h.shape[2]\n",
    "            \n",
    "            # Head-specific sampling\n",
    "            if self.sampling_type == 'random':\n",
    "                # Use head index as seed offset for reproducibility\n",
    "                torch.manual_seed(head_idx * 1000)  # Optional: for reproducibility\n",
    "                rand_idx = torch.randperm(seq_len, device=k_h.device)[:self.num_samples]\n",
    "                q_sample = q_h[:, :, rand_idx]\n",
    "            else:  # spatial\n",
    "                # Head-specific spatial pattern\n",
    "                offset = head_idx * (seq_len // (self.num_heads * 2))\n",
    "                start = max(self.sample_padding, offset % seq_len)\n",
    "                end = seq_len - self.sample_padding - 1\n",
    "                spat_idx = torch.linspace(start, end, self.num_samples, device=k_h.device).long()\n",
    "                q_sample = q_h[:, :, spat_idx]\n",
    "                rand_idx = spat_idx\n",
    "            \n",
    "            # Calculate magnitude matrix\n",
    "            if self.magnitude_type == 'distance':\n",
    "                matrix_magnitude = self._calculate_distance_matrix_N(k_h, q_sample, sqrt=True)\n",
    "            else:\n",
    "                matrix_magnitude = self._calculate_similarity_matrix_N(k_h, q_sample)\n",
    "            \n",
    "            # Set diagonal\n",
    "            range_idx = torch.arange(len(rand_idx), device=k_h.device)\n",
    "            inf_val = float('inf') if self.magnitude_type == 'distance' else float('-inf')\n",
    "            matrix_magnitude[:, rand_idx, range_idx] = inf_val\n",
    "            \n",
    "            prime = self._prime_N(v_h, matrix_magnitude, self.K, rand_idx, self.maximum)\n",
    "        \n",
    "        # Apply head-specific convolution\n",
    "        x = self.conv_heads[head_idx](prime)\n",
    "        \n",
    "        # Remove coordinate channel if needed\n",
    "        if self.coordinate_encoding:\n",
    "            x = self.pointwise_heads[head_idx](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply linear projections\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        \n",
    "        # Split into heads\n",
    "        q_heads = self.split_head(q)  # (B, H, L, D/H)\n",
    "        k_heads = self.split_head(k)  # (B, H, L, D/H)\n",
    "        v_heads = self.split_head(v)  # (B, H, L, D/H)\n",
    "        \n",
    "        # Process each head independently with its own ConvNN\n",
    "        processed_heads = []\n",
    "        for h in range(self.num_heads):\n",
    "            # Get head data and transpose for conv1d\n",
    "            q_h = q_heads[:, h, :, :].transpose(1, 2)  # (B, D/H, L)\n",
    "            k_h = k_heads[:, h, :, :].transpose(1, 2)  # (B, D/H, L)\n",
    "            v_h = v_heads[:, h, :, :].transpose(1, 2)  # (B, D/H, L)\n",
    "            \n",
    "            # Process with head-specific ConvNN\n",
    "            head_output = self.process_head(q_h, k_h, v_h, h)  # (B, D/H, L')\n",
    "            \n",
    "            # Transpose back\n",
    "            head_output = head_output.transpose(1, 2)  # (B, L', D/H)\n",
    "            processed_heads.append(head_output)\n",
    "        \n",
    "        # Stack and combine heads\n",
    "        x = torch.stack(processed_heads, dim=1)  # (B, H, L', D/H)\n",
    "        x = self.combine_heads(x)  # (B, L', D)\n",
    "        \n",
    "        # Apply dropout and output projection\n",
    "        x = self.dropout(x)\n",
    "        x = self.W_o(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Calculation methods (same as original)\n",
    "    def _calculate_similarity_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(k_norm.transpose(2, 1), q_norm)\n",
    "        return torch.clamp(similarity_matrix, min=0)\n",
    "    \n",
    "    def _calculate_similarity_matrix_N(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(k_norm.transpose(2, 1), q_norm)\n",
    "        return torch.clamp(similarity_matrix, min=0)\n",
    "    \n",
    "    def _calculate_distance_matrix(self, K, Q, sqrt=False):\n",
    "        norm_squared_K = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        norm_squared_Q = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(2, 1), Q)\n",
    "        dist_matrix = norm_squared_K + norm_squared_Q.transpose(2, 1) - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0)\n",
    "        return torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "    \n",
    "    def _calculate_distance_matrix_N(self, K, Q, sqrt=False):\n",
    "        norm_squared_K = torch.sum(K**2, dim=1, keepdim=True).permute(0, 2, 1)\n",
    "        norm_squared_Q = torch.sum(Q**2, dim=1, keepdim=True).transpose(2, 1).permute(0, 2, 1)\n",
    "        dot_product = torch.bmm(K.transpose(2, 1), Q)\n",
    "        dist_matrix = norm_squared_K + norm_squared_Q - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0)\n",
    "        return torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "    \n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        _, topk_indices = torch.topk(qk, k=K, dim=-1, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K)\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        return prime.reshape(b, c, -1)\n",
    "    \n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        _, topk_indices = torch.topk(qk, k=K - 1, dim=2, largest=maximum)\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=indices_expanded)\n",
    "        return prime.reshape(b, c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6156b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     41\u001b[39m separate_attention = MultiHeadConvNNAttentionSeparate(\n\u001b[32m     42\u001b[39m     d_hidden=d_hidden,\n\u001b[32m     43\u001b[39m     num_heads=num_heads,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     coordinate_encoding=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Test forward passes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m out_original = \u001b[43moriginal_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m out_enhanced = enhanced_attention(x)\n\u001b[32m     57\u001b[39m out_separate = separate_attention(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Convolutional-Nearest-Neighbor-Attention/layers.py:162\u001b[39m, in \u001b[36mMultiHeadConvNNAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    160\u001b[39m     range_idx = torch.arange(\u001b[38;5;28mlen\u001b[39m(spat_idx), device=q.device)\n\u001b[32m    161\u001b[39m     matrix_magnitude[:, spat_idx, range_idx] = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.magnitude_type == \u001b[33m'\u001b[39m\u001b[33mdistance\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     prime = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prime_N\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix_magnitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspat_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid sampling_type. Must be one of [\u001b[39m\u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspatial\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Convolutional-Nearest-Neighbor-Attention/layers.py:228\u001b[39m, in \u001b[36mMultiHeadConvNNAttention._prime_N\u001b[39m\u001b[34m(self, v, qk, K, rand_idx, maximum)\u001b[39m\n\u001b[32m    226\u001b[39m topk_values_exp = topk_values.unsqueeze(\u001b[32m1\u001b[39m).expand(b, c, t, K-\u001b[32m1\u001b[39m)\n\u001b[32m    227\u001b[39m ones = torch.ones((b, c, t, \u001b[32m1\u001b[39m), device=v.device)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m topk_values_exp = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_values_exp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m v_expanded = v.unsqueeze(-\u001b[32m1\u001b[39m).expand(b, c, t, K).contiguous()\n\u001b[32m    231\u001b[39m prime = torch.gather(v_expanded, dim=\u001b[32m2\u001b[39m, index=indices_expanded)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Test both implementations\n",
    "d_hidden = 768\n",
    "num_heads = 8\n",
    "seq_length = 197\n",
    "batch_size = 4\n",
    "\n",
    "from layers import MultiHeadConvNNAttention\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_length, d_hidden)\n",
    "\n",
    "# Original implementation\n",
    "original_attention = MultiHeadConvNNAttention(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=num_heads,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='spatial',\n",
    "    num_samples=49,\n",
    "    sample_padding=0,\n",
    "    magnitude_type='similarity',\n",
    "    seq_length=seq_length,\n",
    "    coordinate_encoding=True\n",
    ")\n",
    "\n",
    "# Enhanced version with head diversity\n",
    "enhanced_attention = MultiHeadConvNNAttentionEnhanced(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=num_heads,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='spatial',\n",
    "    num_samples=49,\n",
    "    sample_padding=0,\n",
    "    magnitude_type='similarity',\n",
    "    seq_length=seq_length,\n",
    "    coordinate_encoding=True,\n",
    "    diverse_sampling=True  # Enable head-specific sampling\n",
    ")\n",
    "\n",
    "# Separate ConvNN per head version\n",
    "separate_attention = MultiHeadConvNNAttentionSeparate(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=num_heads,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='spatial',\n",
    "    num_samples=49,\n",
    "    sample_padding=0,\n",
    "    magnitude_type='similarity',\n",
    "    seq_length=seq_length,\n",
    "    coordinate_encoding=True\n",
    ")\n",
    "\n",
    "# Test forward passes\n",
    "out_original = original_attention(x)\n",
    "out_enhanced = enhanced_attention(x)\n",
    "out_separate = separate_attention(x)\n",
    "\n",
    "print(f\"Original output shape: {out_original.shape}\")\n",
    "print(f\"Enhanced output shape: {out_enhanced.shape}\")\n",
    "print(f\"Separate output shape: {out_separate.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d04694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hard nearest neighbor selection -> consider using soft selection with temperature\"\"\"\n",
    "\n",
    "def _prime_soft(self, v, qk, K, maximum, temperature=1.0):\n",
    "    b, c, t = v.shape\n",
    "    \n",
    "    # Get top-k values and indices\n",
    "    topk_values, topk_indices = torch.topk(qk, k=K, dim=-1, largest=maximum)\n",
    "    \n",
    "    # Apply softmax to create soft weights\n",
    "    if maximum:  # similarity\n",
    "        weights = F.softmax(topk_values / temperature, dim=-1)\n",
    "    else:  # distance\n",
    "        weights = F.softmax(-topk_values / temperature, dim=-1)\n",
    "    \n",
    "    # Gather and weight\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "    v_gathered = torch.gather(v.unsqueeze(-1).expand(b, c, t, K), \n",
    "                             dim=2, index=topk_indices_exp)\n",
    "    \n",
    "    # Apply soft weights\n",
    "    v_weighted = v_gathered * weights.unsqueeze(1)\n",
    "    prime = v_weighted.reshape(b, c, -1)\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lack of Regularization\"\"\"\n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, ..., weight_decay=1e-4, use_layer_norm=True):\n",
    "        # ...\n",
    "        \n",
    "        # Add layer normalization\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(d_hidden)\n",
    "            \n",
    "        # Add L2 regularization to conv weights\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add residual connection and layer norm\n",
    "        residual = x\n",
    "        \n",
    "        # ... your processing ...\n",
    "        \n",
    "        # Add residual and normalize\n",
    "        x = self.layer_norm(x + residual) if hasattr(self, 'layer_norm') else x\n",
    "        return x\n",
    "        \n",
    "    def get_regularization_loss(self):\n",
    "        \"\"\"Call this during training\"\"\"\n",
    "        reg_loss = 0\n",
    "        for conv in [self.conv]:\n",
    "            reg_loss += self.weight_decay * torch.sum(conv.weight ** 2)\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add noise to the distance/similarity matrices to prevent overfitting to exact patterns\"\"\"\n",
    "def forward(self, x, training=True):\n",
    "    # ...\n",
    "    \n",
    "    if training and self.training:\n",
    "        # Add noise to magnitude matrix\n",
    "        noise = torch.randn_like(matrix_magnitude) * 0.1\n",
    "        matrix_magnitude = matrix_magnitude + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23307089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        # ... existing init ...\n",
    "        \n",
    "        # ADD: Temperature for soft selection\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # ADD: Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(d_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x  # Save for residual connection\n",
    "        \n",
    "        # CRITICAL: Use the linear projections!\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        \n",
    "        # ... rest of your ConvNN processing ...\n",
    "        \n",
    "        # Use soft selection instead of hard topk\n",
    "        # (implement _prime_soft as shown above)\n",
    "        \n",
    "        # CRITICAL: Use W_o projection\n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x.permute(0, 2, 1))))\n",
    "        \n",
    "        # Add residual and layer norm\n",
    "        x = self.layer_norm(x + residual)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
