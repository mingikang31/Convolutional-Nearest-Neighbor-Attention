training: False
_parameters: {}
_buffers: {}
_non_persistent_buffers_set: set()
_backward_pre_hooks: OrderedDict()
_backward_hooks: OrderedDict()
_is_full_backward_hook: None
_forward_hooks: OrderedDict()
_forward_hooks_with_kwargs: OrderedDict()
_forward_hooks_always_called: OrderedDict()
_forward_pre_hooks: OrderedDict()
_forward_pre_hooks_with_kwargs: OrderedDict()
_state_dict_hooks: OrderedDict()
_state_dict_pre_hooks: OrderedDict()
_load_state_dict_pre_hooks: OrderedDict()
_load_state_dict_post_hooks: OrderedDict()
_modules: {'patch_embedding': PatchEmbedding(
  (linear_projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (flatten): Flatten(start_dim=2, end_dim=-1)
), 'positional_encoding': PositionalEncoding(), 'transformer_encoder': Sequential(
  (0): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (1): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (2): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (3): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (4): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (5): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (6): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (7): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (8): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (9): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (10): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
  (11): TransformerEncoder(
    (attention): MultiHeadConvNNAttention(
      (W_q): Linear(in_features=192, out_features=192, bias=False)
      (W_k): Linear(in_features=192, out_features=192, bias=False)
      (W_v): Linear(in_features=192, out_features=192, bias=False)
      (W_o): Linear(in_features=192, out_features=192, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,), groups=192, bias=False)
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=192, out_features=768, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=768, out_features=192, bias=True)
    )
  )
), 'classifier': Linear(in_features=192, out_features=100, bias=True)}
args: Namespace(layer='ConvNNAttention', patch_size=16, num_layers=12, num_heads=1, d_hidden=192, d_mlp=768, dropout=0.1, attention_dropout=0.1, convolution_type='depthwise', softmax_topk_val=True, K=1, sampling_type='random', num_samples=32, sample_padding=0, magnitude_type='matmul', coordinate_encoding=False, branch_ratio=0.5, kernel_size=9, sparse_mode='all', sparse_block_size=32, sparse_context_window=128, dataset='cifar100', resize=224, augment=False, noise=0.0, data_path='./Data', use_compiled=False, compile_mode='default', batch_size=256, num_epochs=150, use_amp=False, clip_grad_norm=1.0, criterion='CrossEntropy', optimizer='adamw', momentum=0.9, weight_decay=0.01, lr=0.0001, lr_step=20, lr_gamma=0.1, scheduler='none', device='cuda', seed=42, output_dir='./Final_Output/K_test_correct_random/ViT-Tiny-CIFAR100/ConvNNAttention_Random_K1_NS32_s42', test_only=False, num_classes=100, img_size=(3, 224, 224), model='VIT', total_params=5498980, trainable_params=5498980)
model: VIT
d_hidden: 192
d_mlp: 768
img_size: (224, 224)
n_classes: 100
n_heads: 1
patch_size: (16, 16)
n_channels: 3
n_layers: 12
n_patches: 196
dropout: 0.1
attention_dropout: 0.1
max_seq_length: 197
device: cuda
name: VIT ConvNNAttention
