layer: ConvNNAttention
patch_size: 16
num_layers: 12
num_heads: 1
d_hidden: 192
d_mlp: 768
dropout: 0.1
attention_dropout: 0.1
convolution_type: depthwise
softmax_topk_val: True
K: 8
sampling_type: random
num_samples: 32
sample_padding: 0
magnitude_type: matmul
coordinate_encoding: False
branch_ratio: 0.5
kernel_size: 9
sparse_mode: all
sparse_block_size: 32
sparse_context_window: 128
dataset: cifar10
resize: 224
augment: False
noise: 0.0
data_path: ./Data
use_compiled: False
compile_mode: default
batch_size: 256
num_epochs: 150
use_amp: False
clip_grad_norm: 1.0
criterion: CrossEntropy
optimizer: adamw
momentum: 0.9
weight_decay: 0.01
lr: 0.0001
lr_step: 20
lr_gamma: 0.1
scheduler: none
device: cuda
seed: 42
output_dir: ./Final_Output/K_test_correct_random/ViT-Tiny-CIFAR10/ConvNNAttention_Random_K8_NS32_s42
test_only: False
num_classes: 10
img_size: (3, 224, 224)
model: VIT
total_params: 5497738
trainable_params: 5497738
