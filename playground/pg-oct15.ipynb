{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7a4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593c601",
   "metadata": {},
   "source": [
    "### 1. Regular Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfb0307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Multi-Head Layers for Transformer Encoder\"\"\"\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_hidden, num_heads, attention_dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_hidden // num_heads # dimension of each head\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        print(f\"[Attention Scores]: {attn_scores.shape} \\n {attn_scores} \\n\")\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))\n",
    "        print(f\"[Attention Probs]: {attn_probs.shape} \\n {attn_probs} \\n\")\n",
    "\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        print(f\"[Attention Output]: {output.shape} \\n {output} \\n\")\n",
    "\n",
    "        return output, attn_probs\n",
    "    \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        q = self.split_head(self.W_q(x)) # (B, num_heads, seq_length, d_k)\n",
    "        k = self.split_head(self.W_k(x))\n",
    "        v = self.split_head(self.W_v(x))\n",
    "        \n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask) # (B, num_heads, seq_length, d_k)\n",
    "        output = self.W_o(self.combine_heads(attn_output)) # (B, seq_length, d_hidden)\n",
    "        print(f\"[Final Output]: {output.shape} \\n {output} \\n\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdaaa8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 6, 8]) \n",
      " tensor([[[-0.8775, -2.1730,  0.7890,  0.5044, -0.9170,  0.6572, -0.0923,\n",
      "           0.3994],\n",
      "         [-1.8758, -0.8411,  1.3538, -2.2346, -0.8308,  2.4109,  1.1988,\n",
      "          -0.5029],\n",
      "         [ 0.6618,  1.5232,  1.5176, -0.3011, -0.5179, -1.2367,  1.1713,\n",
      "           0.5475],\n",
      "         [ 1.4358, -1.6689, -0.1532,  0.3160,  0.5740, -0.6418,  0.3470,\n",
      "           1.1666],\n",
      "         [-1.0268, -0.3807,  0.1988,  1.6391,  0.0255,  0.5719, -0.3235,\n",
      "          -0.5822],\n",
      "         [-0.5843, -0.5342, -0.5527,  0.3054, -0.0530, -0.1742, -1.4902,\n",
      "          -0.5062]]]) \n",
      "\n",
      "[Attention Scores]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[  8.2686,   6.3918, -16.2770,  -6.6518,  -0.5907,  17.3586],\n",
      "          [  6.3918,   4.9409, -12.5823,  -5.1419,  -0.4566,  13.4184],\n",
      "          [-16.2770, -12.5823,  32.0414,  13.0941,   1.1628, -34.1707],\n",
      "          [ -6.6518,  -5.1419,  13.0941,   5.3511,   0.4752, -13.9643],\n",
      "          [ -0.5907,  -0.4566,   1.1628,   0.4752,   0.0422,  -1.2401],\n",
      "          [ 17.3586,  13.4184, -34.1707, -13.9643,  -1.2401,  36.4415]]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "[Attention Probs]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[1.1278e-04, 1.7262e-05, 2.4671e-15, 3.7355e-11, 1.6019e-08,\n",
      "           9.9987e-01],\n",
      "          [8.8691e-04, 2.0787e-04, 5.0997e-12, 8.6868e-09, 9.4115e-07,\n",
      "           9.9890e-01],\n",
      "          [1.0366e-21, 4.1702e-20, 1.0000e+00, 5.9060e-09, 3.8868e-14,\n",
      "           1.7556e-29],\n",
      "          [2.6562e-09, 1.2023e-08, 9.9956e-01, 4.3356e-04, 3.3073e-06,\n",
      "           1.7721e-12],\n",
      "          [7.5601e-02, 8.6449e-02, 4.3659e-01, 2.1951e-01, 1.4236e-01,\n",
      "           3.9491e-02],\n",
      "          [5.1572e-09, 1.0028e-10, 2.1553e-31, 1.2854e-22, 4.3161e-17,\n",
      "           1.0000e+00]]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "[Attention Output]: torch.Size([1, 1, 6, 8]) \n",
      " tensor([[[[-3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892,\n",
      "           -3.5892],\n",
      "          [-3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873,\n",
      "           -3.5873],\n",
      "          [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "            3.3658],\n",
      "          [ 3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,\n",
      "            3.3649],\n",
      "          [ 1.4035,  1.4035,  1.4035,  1.4035,  1.4035,  1.4035,  1.4035,\n",
      "            1.4035],\n",
      "          [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "           -3.5894]]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "[Final Output]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134,\n",
      "          -28.7134],\n",
      "         [-28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983,\n",
      "          -28.6983],\n",
      "         [ 26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,\n",
      "           26.9261],\n",
      "         [ 26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,\n",
      "           26.9191],\n",
      "         [ 11.2280,  11.2280,  11.2280,  11.2280,  11.2280,  11.2280,  11.2280,\n",
      "           11.2280],\n",
      "         [-28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155,\n",
      "          -28.7155]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 8 \n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 1\n",
    "dropout = 0.0 \n",
    "\n",
    "# x = torch.Tensor(\n",
    "#     [\n",
    "#         [\n",
    "#             [ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "#             [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "#             [17, 18, 19, 20, 21, 22, 23, 24],\n",
    "#             [25, 26, 27, 28, 29, 30, 31, 32],\n",
    "#             [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "#             [41, 42, 43, 44, 45, 46, 47, 48],\n",
    "#         ]\n",
    "#     ]\n",
    "# ).to(torch.float32)\n",
    "\n",
    "# x = torch.randint(0, 10, (batch_size, seq_length, d_hidden)).to(torch.float32)\n",
    "x = torch.randn(batch_size, seq_length, d_hidden).to(torch.float32)\n",
    "\n",
    "print(f\"Input: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "attention = MultiHeadAttention(d_hidden=d_hidden, num_heads=num_heads, attention_dropout=dropout)\n",
    "attention.W_k.weight.data.fill_(1.0)\n",
    "attention.W_q.weight.data.fill_(1.0)\n",
    "attention.W_v.weight.data.fill_(1.0)\n",
    "attention.W_o.weight.data.fill_(1.0)\n",
    "out = attention(x)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d509c3",
   "metadata": {},
   "source": [
    "## IMPORTANT ConvNN-Attention == KvT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc86727",
   "metadata": {},
   "source": [
    "### 2. Convolutional Nearest Neighbor Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50fa618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else (d_hidden // num_heads)\n",
    "        self.out_channels = (d_hidden // num_heads) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "            bias = False, \n",
    "            groups=self.in_channels\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1.1\n",
    "        self.NEG_INF = -0.1 \n",
    "\n",
    "    \"\"\"K, Q, V projection functions\"\"\"\n",
    "    def split_head(self, x): ## K, Q, V\n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "    def batch_combine(self, x):  ## K, Q, V\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "\n",
    "    \"\"\"Output projection function\"\"\"\n",
    "    \n",
    "    # def batch_split(self, x): ## O\n",
    "    #     x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "    #     return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    # def combine_heads(self, x): ## O\n",
    "    #     batch_size, _, seq_length, d_k = x.size()\n",
    "    #     return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "\n",
    "    def batch_split(self, x):\n",
    "        if self.num_heads == 1:\n",
    "            return x.unsqueeze(1)  # Just add head dimension [B, 1, seq_len, dim]\n",
    "        else:\n",
    "            x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "            return x.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        if self.num_heads == 1:\n",
    "            return x.squeeze(1)  # Just remove head dimension\n",
    "        else:\n",
    "            batch_size, _, seq_length, d_k = x.size()\n",
    "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.batch_combine(self.split_head(self.W_k(x)))\n",
    "        v = self.batch_combine(self.split_head(self.W_v(x)))\n",
    "        q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "        print(f\"[q shape]: {q.shape} \\n {q} \\n\")\n",
    "        print(f\"[q transpose shape]: {q.transpose(1, 2).shape} \\n {q.transpose(1, 2)} \\n\")\n",
    "        print(f\"[k shape]: {k.shape} \\n {k} \\n\")\n",
    "        print(f\"[v shape]: {v.shape} \\n {v} \\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        similarity_matrix = self._calculate_attention_matrix(k, q)\n",
    "        print(f\"[Attention Score]: {similarity_matrix.shape} \\n {similarity_matrix} \\n\")\n",
    "\n",
    "        # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "        prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        print(f\"[After Conv1d]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        print(f\"[After Dropout + Permute]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "        # # 6. Final Linear Projection\n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x)))\n",
    "        # x = self.W_o(x)\n",
    "        print(f\"[After projection]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "        print(f\"[After projection]: {x.shape} \\n {x} \\n\")\n",
    "        \n",
    "        return x       \n",
    "    def _calculate_attention_matrix(self, K, Q):\n",
    "        attn_score = torch.matmul(K.transpose(1, 2), Q) / self.d_k**0.5\n",
    "        return attn_score\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=True)\n",
    "        print(f\"[Top-{K} Indices]: {topk_indices.shape} \\n {topk_indices} \\n\")\n",
    "        print(f\"[Top-{K} Values]: {topk_values.shape} \\n {topk_values} \\n\")\n",
    "\n",
    "        topk_values = torch.softmax(topk_values, dim=-1)\n",
    "        print(f\"[After Softmax Top-K Values]: {topk_values.shape} \\n {topk_values} \\n\")\n",
    "        \n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        \n",
    "        prime = topk_values_exp * prime \n",
    "        \n",
    "        prime = prime.view(b, c, -1)\n",
    "        print(f\"[Prime]: {prime.shape} \\n {prime} \\n\")\n",
    "\n",
    "        return prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c08c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input x]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-0.8775, -2.1730,  0.7890,  0.5044, -0.9170,  0.6572, -0.0923,\n",
      "           0.3994],\n",
      "         [-1.8758, -0.8411,  1.3538, -2.2346, -0.8308,  2.4109,  1.1988,\n",
      "          -0.5029],\n",
      "         [ 0.6618,  1.5232,  1.5176, -0.3011, -0.5179, -1.2367,  1.1713,\n",
      "           0.5475],\n",
      "         [ 1.4358, -1.6689, -0.1532,  0.3160,  0.5740, -0.6418,  0.3470,\n",
      "           1.1666],\n",
      "         [-1.0268, -0.3807,  0.1988,  1.6391,  0.0255,  0.5719, -0.3235,\n",
      "          -0.5822],\n",
      "         [-0.5843, -0.5342, -0.5527,  0.3054, -0.0530, -0.1742, -1.4902,\n",
      "          -0.5062]]]) \n",
      "\n",
      "[q shape]: torch.Size([1, 8, 6]) \n",
      " tensor([[[-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894]]],\n",
      "       grad_fn=<ViewBackward0>) \n",
      "\n",
      "[q transpose shape]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098,\n",
      "          -1.7098],\n",
      "         [-1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217,\n",
      "          -1.3217],\n",
      "         [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "           3.3658],\n",
      "         [ 1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,\n",
      "           1.3755],\n",
      "         [ 0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,\n",
      "           0.1221],\n",
      "         [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "          -3.5894]]], grad_fn=<TransposeBackward0>) \n",
      "\n",
      "[k shape]: torch.Size([1, 8, 6]) \n",
      " tensor([[[-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894]]],\n",
      "       grad_fn=<ViewBackward0>) \n",
      "\n",
      "[v shape]: torch.Size([1, 8, 6]) \n",
      " tensor([[[-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894],\n",
      "         [-1.7098, -1.3217,  3.3658,  1.3755,  0.1221, -3.5894]]],\n",
      "       grad_fn=<ViewBackward0>) \n",
      "\n",
      "[Attention Score]: torch.Size([1, 6, 6]) \n",
      " tensor([[[  8.2686,   6.3918, -16.2770,  -6.6518,  -0.5907,  17.3586],\n",
      "         [  6.3918,   4.9409, -12.5823,  -5.1419,  -0.4566,  13.4184],\n",
      "         [-16.2770, -12.5823,  32.0414,  13.0941,   1.1628, -34.1707],\n",
      "         [ -6.6518,  -5.1419,  13.0941,   5.3511,   0.4752, -13.9643],\n",
      "         [ -0.5907,  -0.4566,   1.1628,   0.4752,   0.0422,  -1.2401],\n",
      "         [ 17.3586,  13.4184, -34.1707, -13.9643,  -1.2401,  36.4415]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "[Top-3 Indices]: torch.Size([1, 6, 3]) \n",
      " tensor([[[5, 0, 1],\n",
      "         [5, 0, 1],\n",
      "         [2, 3, 4],\n",
      "         [2, 3, 4],\n",
      "         [2, 3, 4],\n",
      "         [5, 0, 1]]]) \n",
      "\n",
      "[Top-3 Values]: torch.Size([1, 6, 3]) \n",
      " tensor([[[17.3586,  8.2686,  6.3918],\n",
      "         [13.4184,  6.3918,  4.9409],\n",
      "         [32.0414, 13.0941,  1.1628],\n",
      "         [13.0941,  5.3511,  0.4752],\n",
      "         [ 1.1628,  0.4752,  0.0422],\n",
      "         [36.4415, 17.3586, 13.4184]]], grad_fn=<TopkBackward0>) \n",
      "\n",
      "[After Softmax Top-K Values]: torch.Size([1, 6, 3]) \n",
      " tensor([[[9.9987e-01, 1.1278e-04, 1.7262e-05],\n",
      "         [9.9891e-01, 8.8691e-04, 2.0787e-04],\n",
      "         [1.0000e+00, 5.9060e-09, 3.8868e-14],\n",
      "         [9.9956e-01, 4.3356e-04, 3.3073e-06],\n",
      "         [5.4679e-01, 2.7491e-01, 1.7830e-01],\n",
      "         [1.0000e+00, 5.1572e-09, 1.0028e-10]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "[Prime]: torch.Size([1, 8, 18]) \n",
      " tensor([[[-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10],\n",
      "         [-3.5890e+00, -1.9282e-04, -2.2815e-05, -3.5855e+00, -1.5164e-03,\n",
      "          -2.7473e-04,  3.3658e+00,  8.1235e-09,  4.7476e-15,  3.3643e+00,\n",
      "           5.9634e-04,  4.0398e-07,  1.8404e+00,  3.7813e-01,  2.1778e-02,\n",
      "          -3.5894e+00, -8.8177e-09, -1.3254e-10]]], grad_fn=<ViewBackward0>) \n",
      "\n",
      "[After Conv1d]: torch.Size([1, 8, 6]) \n",
      " tensor([[[-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894],\n",
      "         [-3.5892, -3.5873,  3.3658,  3.3649,  2.2403, -3.5894]]],\n",
      "       grad_fn=<ConvolutionBackward0>) \n",
      "\n",
      "[After Dropout + Permute]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892,\n",
      "          -3.5892],\n",
      "         [-3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873,\n",
      "          -3.5873],\n",
      "         [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "           3.3658],\n",
      "         [ 3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,\n",
      "           3.3649],\n",
      "         [ 2.2403,  2.2403,  2.2403,  2.2403,  2.2403,  2.2403,  2.2403,\n",
      "           2.2403],\n",
      "         [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "          -3.5894]]], grad_fn=<PermuteBackward0>) \n",
      "\n",
      "[After projection]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134,\n",
      "          -28.7134],\n",
      "         [-28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983,\n",
      "          -28.6983],\n",
      "         [ 26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,\n",
      "           26.9261],\n",
      "         [ 26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,\n",
      "           26.9191],\n",
      "         [ 17.9222,  17.9222,  17.9222,  17.9222,  17.9222,  17.9222,  17.9222,\n",
      "           17.9222],\n",
      "         [-28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155,\n",
      "          -28.7155]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "[After projection]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134,\n",
      "          -28.7134],\n",
      "         [-28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983,\n",
      "          -28.6983],\n",
      "         [ 26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,\n",
      "           26.9261],\n",
      "         [ 26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,\n",
      "           26.9191],\n",
      "         [ 17.9222,  17.9222,  17.9222,  17.9222,  17.9222,  17.9222,  17.9222,\n",
      "           17.9222],\n",
      "         [-28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155,\n",
      "          -28.7155]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 8 \n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 1\n",
    "dropout = 0.0 \n",
    "\n",
    "# x = torch.Tensor(\n",
    "#     [\n",
    "#         [\n",
    "#             [ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "#             [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "#             [17, 18, 19, 20, 21, 22, 23, 24],\n",
    "#             [25, 26, 27, 28, 29, 30, 31, 32],\n",
    "#             [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "#             [41, 42, 43, 44, 45, 46, 47, 48],\n",
    "#         ]\n",
    "#     ]\n",
    "# ).to(torch.float32)\n",
    "\n",
    "print(f\"[Input x]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "convnn = MultiHeadConvNNAttention(d_hidden=d_hidden, \n",
    "                                  num_heads=num_heads, \n",
    "                                  attention_dropout=dropout, \n",
    "                                  K=3,\n",
    "                                  sampling_type='all',\n",
    "                                  num_samples=-1,\n",
    "                                  sample_padding=0,\n",
    "                                  magnitude_type='cosine',\n",
    "                                  coordinate_encoding=False,\n",
    "                                  seq_length=6\n",
    "                                  )\n",
    "\n",
    "convnn.W_k.weight.data.fill_(1.0)\n",
    "convnn.W_q.weight.data.fill_(1.0)\n",
    "convnn.W_v.weight.data.fill_(1.0)\n",
    "convnn.W_o.weight.data.fill_(1.0)\n",
    "convnn.conv.weight.data.fill_(1.0)\n",
    "out = convnn(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ff7b6",
   "metadata": {},
   "source": [
    "### 3. KvT Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf752a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadKvtAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,topk=100):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "    \n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.topk = topk\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        print(f\"[q shape]: {q.shape} \\n {q} \\n\")\n",
    "        print(f\"[k shape]: {k.shape} \\n {k} \\n\")\n",
    "        print(f\"[v shape]: {v.shape} \\n {v} \\n\")\n",
    "                \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        print(f\"[Attention Score]: {attn.shape} \\n {attn} \\n\")\n",
    "        \n",
    "        # the core code block\n",
    "        mask=torch.zeros(B,self.num_heads,N,N,device=x.device,requires_grad=False)\n",
    "        print(f\"[Attention mask initialization]: {mask.shape} \\n {mask} \\n\")\n",
    "\n",
    "        index=torch.topk(attn,k=self.topk,dim=-1,largest=True)[1]\n",
    "        print(f\"[Top-k indices]: {index.shape} \\n {index} \\n\")\n",
    "\n",
    "        mask.scatter_(-1,index,1.)\n",
    "        print(f\"[Attention mask after scatter]: {mask.shape} \\n {mask} \\n\")\n",
    "        \n",
    "        attn=torch.where(mask>0,attn,torch.full_like(attn,float('-inf')))\n",
    "        print(f\"[Attention Score after masking]: {attn.shape} \\n {attn} \\n\")\n",
    "\n",
    "        # end of the core code block\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        print(f\"[Attention Probability]: {attn.shape} \\n {attn} \\n\")\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v)\n",
    "        print(f\"[x after attn @ v]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        print(f\"[x after transpose and reshape]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "        x = self.proj(x)\n",
    "        print(f\"[After projection]: {x.shape} \\n {x} \\n\")\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c9df1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input x]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-0.8775, -2.1730,  0.7890,  0.5044, -0.9170,  0.6572, -0.0923,\n",
      "           0.3994],\n",
      "         [-1.8758, -0.8411,  1.3538, -2.2346, -0.8308,  2.4109,  1.1988,\n",
      "          -0.5029],\n",
      "         [ 0.6618,  1.5232,  1.5176, -0.3011, -0.5179, -1.2367,  1.1713,\n",
      "           0.5475],\n",
      "         [ 1.4358, -1.6689, -0.1532,  0.3160,  0.5740, -0.6418,  0.3470,\n",
      "           1.1666],\n",
      "         [-1.0268, -0.3807,  0.1988,  1.6391,  0.0255,  0.5719, -0.3235,\n",
      "          -0.5822],\n",
      "         [-0.5843, -0.5342, -0.5527,  0.3054, -0.0530, -0.1742, -1.4902,\n",
      "          -0.5062]]]) \n",
      "\n",
      "[q shape]: torch.Size([1, 1, 6, 8]) \n",
      " tensor([[[[-1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098,\n",
      "           -1.7098],\n",
      "          [-1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217,\n",
      "           -1.3217],\n",
      "          [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "            3.3658],\n",
      "          [ 1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,\n",
      "            1.3755],\n",
      "          [ 0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,\n",
      "            0.1221],\n",
      "          [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "           -3.5894]]]], grad_fn=<SelectBackward0>) \n",
      "\n",
      "[k shape]: torch.Size([1, 1, 6, 8]) \n",
      " tensor([[[[-1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098,\n",
      "           -1.7098],\n",
      "          [-1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217,\n",
      "           -1.3217],\n",
      "          [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "            3.3658],\n",
      "          [ 1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,\n",
      "            1.3755],\n",
      "          [ 0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,\n",
      "            0.1221],\n",
      "          [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "           -3.5894]]]], grad_fn=<SelectBackward0>) \n",
      "\n",
      "[v shape]: torch.Size([1, 1, 6, 8]) \n",
      " tensor([[[[-1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098, -1.7098,\n",
      "           -1.7098],\n",
      "          [-1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217, -1.3217,\n",
      "           -1.3217],\n",
      "          [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "            3.3658],\n",
      "          [ 1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,  1.3755,\n",
      "            1.3755],\n",
      "          [ 0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,  0.1221,\n",
      "            0.1221],\n",
      "          [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "           -3.5894]]]], grad_fn=<SelectBackward0>) \n",
      "\n",
      "[Attention Score]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[  8.2686,   6.3918, -16.2770,  -6.6518,  -0.5907,  17.3586],\n",
      "          [  6.3918,   4.9409, -12.5823,  -5.1419,  -0.4566,  13.4184],\n",
      "          [-16.2770, -12.5823,  32.0414,  13.0941,   1.1628, -34.1707],\n",
      "          [ -6.6518,  -5.1419,  13.0941,   5.3511,   0.4752, -13.9643],\n",
      "          [ -0.5907,  -0.4566,   1.1628,   0.4752,   0.0422,  -1.2401],\n",
      "          [ 17.3586,  13.4184, -34.1707, -13.9643,  -1.2401,  36.4415]]]],\n",
      "       grad_fn=<MulBackward0>) \n",
      "\n",
      "[Attention mask initialization]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]]]]) \n",
      "\n",
      "[Top-k indices]: torch.Size([1, 1, 6, 3]) \n",
      " tensor([[[[5, 0, 1],\n",
      "          [5, 0, 1],\n",
      "          [2, 3, 4],\n",
      "          [2, 3, 4],\n",
      "          [2, 3, 4],\n",
      "          [5, 0, 1]]]]) \n",
      "\n",
      "[Attention mask after scatter]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[1., 1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 0., 0., 1.],\n",
      "          [0., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 0., 0., 1.]]]]) \n",
      "\n",
      "[Attention Score after masking]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[ 8.2686,  6.3918,    -inf,    -inf,    -inf, 17.3586],\n",
      "          [ 6.3918,  4.9409,    -inf,    -inf,    -inf, 13.4184],\n",
      "          [   -inf,    -inf, 32.0414, 13.0941,  1.1628,    -inf],\n",
      "          [   -inf,    -inf, 13.0941,  5.3511,  0.4752,    -inf],\n",
      "          [   -inf,    -inf,  1.1628,  0.4752,  0.0422,    -inf],\n",
      "          [17.3586, 13.4184,    -inf,    -inf,    -inf, 36.4415]]]],\n",
      "       grad_fn=<WhereBackward0>) \n",
      "\n",
      "[Attention Probability]: torch.Size([1, 1, 6, 6]) \n",
      " tensor([[[[1.1278e-04, 1.7262e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           9.9987e-01],\n",
      "          [8.8691e-04, 2.0787e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           9.9891e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 1.0000e+00, 5.9060e-09, 3.8868e-14,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 9.9956e-01, 4.3356e-04, 3.3073e-06,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 5.4679e-01, 2.7491e-01, 1.7830e-01,\n",
      "           0.0000e+00],\n",
      "          [5.1572e-09, 1.0028e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           1.0000e+00]]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "[x after attn @ v]: torch.Size([1, 1, 6, 8]) \n",
      " tensor([[[[-3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892,\n",
      "           -3.5892],\n",
      "          [-3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873,\n",
      "           -3.5873],\n",
      "          [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "            3.3658],\n",
      "          [ 3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,\n",
      "            3.3649],\n",
      "          [ 2.2403,  2.2403,  2.2403,  2.2403,  2.2403,  2.2403,  2.2403,\n",
      "            2.2403],\n",
      "          [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "           -3.5894]]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "[x after transpose and reshape]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892, -3.5892,\n",
      "          -3.5892],\n",
      "         [-3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873, -3.5873,\n",
      "          -3.5873],\n",
      "         [ 3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,  3.3658,\n",
      "           3.3658],\n",
      "         [ 3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,  3.3649,\n",
      "           3.3649],\n",
      "         [ 2.2403,  2.2403,  2.2403,  2.2403,  2.2403,  2.2403,  2.2403,\n",
      "           2.2403],\n",
      "         [-3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894, -3.5894,\n",
      "          -3.5894]]], grad_fn=<ViewBackward0>) \n",
      "\n",
      "[After projection]: torch.Size([1, 6, 8]) \n",
      " tensor([[[-28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134, -28.7134,\n",
      "          -28.7134],\n",
      "         [-28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983, -28.6983,\n",
      "          -28.6983],\n",
      "         [ 26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,  26.9261,\n",
      "           26.9261],\n",
      "         [ 26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,  26.9191,\n",
      "           26.9191],\n",
      "         [ 17.9222,  17.9222,  17.9222,  17.9222,  17.9222,  17.9222,  17.9222,\n",
      "           17.9222],\n",
      "         [-28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155, -28.7155,\n",
      "          -28.7155]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 8 \n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 1\n",
    "dropout = 0.0 \n",
    "\n",
    "# x = torch.Tensor(\n",
    "#     [\n",
    "#         [\n",
    "#             [ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "#             [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "#             [17, 18, 19, 20, 21, 22, 23, 24],\n",
    "#             [25, 26, 27, 28, 29, 30, 31, 32],\n",
    "#             [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "#             [41, 42, 43, 44, 45, 46, 47, 48],\n",
    "#         ]\n",
    "#     ]\n",
    "# ).to(torch.float32)\n",
    "\n",
    "print(f\"[Input x]: {x.shape} \\n {x} \\n\")\n",
    "\n",
    "kvt = MultiHeadKvtAttention(d_hidden,\n",
    "                               num_heads, \n",
    "                               qkv_bias=False,\n",
    "                               qk_scale=None,\n",
    "                               attn_drop=dropout,\n",
    "                               proj_drop=dropout,\n",
    "                               topk=3)\n",
    "\n",
    "kvt.qkv.weight.data.fill_(1.0)\n",
    "kvt.proj.weight.data.fill_(1.0)\n",
    "out = kvt(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60a374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
