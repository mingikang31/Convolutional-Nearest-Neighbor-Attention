{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25adeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6156b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test both implementations\n",
    "d_hidden = 768\n",
    "num_heads = 8\n",
    "seq_length = 197\n",
    "batch_size = 4\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_length, d_hidden)\n",
    "print(\"Input shape:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a564b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "\n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else d_hidden // num_heads\n",
    "        self.out_channels = (d_hidden // num_heads) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1e5 \n",
    "        self.NEG_INF = -1e5\n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.batch_combine(self.split_head(self.W_k(x)))\n",
    "        v = self.batch_combine(self.split_head(self.W_v(x)))\n",
    "\n",
    "        print(\"k shape after split and combine:\", k.shape)\n",
    "\n",
    "        # 3. Add Coordinate Encoding \n",
    "        k = self._add_coordinate_encoding(k) if self.coordinate_encoding else k\n",
    "        v = self._add_coordinate_encoding(v) if self.coordinate_encoding else v\n",
    "\n",
    "        print(\"k shape after encoding:\", k.shape)\n",
    "\n",
    "        \n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "            print(\"q shape: \", q.shape)\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix(k, q, sqrt=True)\n",
    "            print(\"similarity_marix: \", similarity_matrix.shape)\n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "            print(\"prime: \", prime.shape)\n",
    "\n",
    "        elif self.sampling_type == 'random': # Random Samples\n",
    "            rand_idx = torch.randperm(x.shape[1], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, rand_idx, :]\n",
    "            print(\"x sample shape: \", x_sample.shape)\n",
    "            \n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "            print(\"q shape: \", q.shape)\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "\n",
    "            print(\"similarity_marix: \", similarity_matrix.shape)\n",
    "            range_idx = torch.arange(len(rand_idx), device=q.device)\n",
    "            similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "\n",
    "            print(\"prime: \", prime.shape)\n",
    "        elif self.sampling_type == 'spatial': # Spatial Samples\n",
    "            spat_idx = torch.linspace(0 + self.sample_padding, x.shape[1] - self.sample_padding - 1, self.num_samples, device=x.device).long()\n",
    "            x_sample = x[:, spat_idx, :]\n",
    "            print(\"x sample shape: \", x_sample.shape)\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "            print(\"q shape: \", q.shape)\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "\n",
    "            print(\"similarity_marix: \", similarity_matrix.shape)\n",
    "            range_idx = torch.arange(len(spat_idx), device=q.device)\n",
    "            similarity_matrix[:, spat_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, spat_idx, self.maximum)\n",
    "\n",
    "            print(\"prime: \", prime.shape)\n",
    "\n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial']\")\n",
    "\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x)))\n",
    "        return x       \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1)  # Fill diagonal with -0.1 to avoid self-selection\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime \n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indicies back to original matrix positions \n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=-1)\n",
    "        topk_indices_exp = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Expand topk values to match the shape of indices\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        ones = torch.ones((b, c, t, 1), device=v.device)\n",
    "        topk_values_exp = torch.cat((ones, topk_values_exp), dim=-1)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting \n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()    \n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, c, t = x.shape \n",
    "        cache_key = f\"{b}_{t}_{x.device}\"\n",
    "        if cache_key in self.coordinate_cache: \n",
    "            expanded_coords = self.coordinate_cache[cache_key]\n",
    "        else: \n",
    "            coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1) \n",
    "            expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1) \n",
    "            self.coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "        x_with_coords = torch.cat([x, expanded_coords], dim=1) \n",
    "        return x_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4e660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n",
      "k shape after split and combine: torch.Size([32, 96, 197])\n",
      "k shape after encoding: torch.Size([32, 97, 197])\n",
      "q shape:  torch.Size([32, 97, 197])\n",
      "similarity_marix:  torch.Size([32, 197, 197])\n",
      "prime:  torch.Size([32, 97, 788])\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "convnn= MultiHeadConvNNAttention(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=8,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='all',  # 'all', 'random', 'spatial'\n",
    "    num_samples=-1, \n",
    "    sample_padding=0, \n",
    "    magnitude_type='euclidean',  # 'euclidean' or 'cosine'\n",
    "    coordinate_encoding=True\n",
    ")\n",
    "ex = convnn(x)\n",
    "print(\"Output shape:\", ex.shape)  # Expected: (B, seq_length, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29581367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n",
      "k shape after split and combine: torch.Size([32, 96, 197])\n",
      "k shape after encoding: torch.Size([32, 97, 197])\n",
      "x sample shape:  torch.Size([4, 30, 768])\n",
      "q shape:  torch.Size([32, 97, 30])\n",
      "similarity_marix:  torch.Size([32, 197, 30])\n",
      "prime:  torch.Size([32, 97, 788])\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "convnn= MultiHeadConvNNAttention(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=8,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='random',  # 'all', 'random', 'spatial'\n",
    "    num_samples=30, \n",
    "    sample_padding=0, \n",
    "    magnitude_type='euclidean',  # 'euclidean' or 'cosine'\n",
    "    coordinate_encoding=True\n",
    ")\n",
    "ex = convnn(x)\n",
    "print(\"Output shape:\", ex.shape)  # Expected: (B, seq_length, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef7f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n",
      "k shape after split and combine: torch.Size([32, 96, 197])\n",
      "k shape after encoding: torch.Size([32, 97, 197])\n",
      "x sample shape:  torch.Size([4, 30, 768])\n",
      "q shape:  torch.Size([32, 97, 30])\n",
      "similarity_marix:  torch.Size([32, 197, 30])\n",
      "prime:  torch.Size([32, 97, 788])\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "convnn= MultiHeadConvNNAttention(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=8,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='spatial',  # 'all', 'random', 'spatial'\n",
    "    num_samples=30, \n",
    "    sample_padding=0, \n",
    "    magnitude_type='euclidean',  # 'euclidean' or 'cosine'\n",
    "    coordinate_encoding=True\n",
    ")\n",
    "ex = convnn(x)\n",
    "print(\"Output shape:\", ex.shape)  # Expected: (B, seq_length, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505833d4",
   "metadata": {},
   "source": [
    "## Remove batch split, batch combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa5607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadConvNNAttention_NoBatchSplit(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention_NoBatchSplit, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        # self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else d_hidden // num_heads\n",
    "        # self.out_channels = (d_hidden // num_heads) \n",
    "        self.in_channels = d_hidden + 1 if coordinate_encoding else d_hidden\n",
    "        self.out_channels = d_hidden\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1e5 \n",
    "        self.NEG_INF = -1e5\n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.W_k(x) \n",
    "        v = self.W_v(x) \n",
    "\n",
    "        print(\"k shape after linear:\", k.shape)\n",
    "        print(\"v shape after linear:\", v.shape)\n",
    "        k = k.transpose(1, 2)   \n",
    "        v = v.transpose(1, 2)\n",
    "        print(\"k shape after transpose:\", k.shape)\n",
    "        print(\"v shape after transpose:\", v.shape)\n",
    "\n",
    "        # k = self.batch_combine(self.split_head(k))\n",
    "        # v = self.batch_combine(self.split_head(v))\n",
    "\n",
    "\n",
    "        # 2. Add Coordinate Encoding \n",
    "        k = self._add_coordinate_encoding(k) if self.coordinate_encoding else k\n",
    "        v = self._add_coordinate_encoding(v) if self.coordinate_encoding else v\n",
    "\n",
    "\n",
    "        # 3. Sampling & Similarity Calculation\n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            # q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "            q = self.W_q(x)\n",
    "            q = q.transpose(1, 2)\n",
    "            \n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix(k, q, sqrt=True)\n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'random': # Random Samples\n",
    "            rand_idx = torch.randperm(x.shape[1], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, rand_idx, :]            \n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(rand_idx), device=q.device)\n",
    "            similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'spatial': # Spatial Samples\n",
    "            spat_idx = torch.linspace(0 + self.sample_padding, x.shape[1] - self.sample_padding - 1, self.num_samples, device=x.device).long()\n",
    "            x_sample = x[:, spat_idx, :]\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(spat_idx), device=q.device)\n",
    "            similarity_matrix[:, spat_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, spat_idx, self.maximum)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial']\")\n",
    "\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        # 6. Final Linear Projection\n",
    "        x = self.W_o(x)\n",
    "        return x       \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1)  # Fill diagonal with -0.1 to avoid self-selection\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime \n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indicies back to original matrix positions \n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=-1)\n",
    "        topk_indices_exp = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Expand topk values to match the shape of indices\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        ones = torch.ones((b, c, t, 1), device=v.device)\n",
    "        topk_values_exp = torch.cat((ones, topk_values_exp), dim=-1)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting \n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()    \n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, c, t = x.shape \n",
    "        cache_key = f\"{b}_{t}_{x.device}\"\n",
    "        if cache_key in self.coordinate_cache: \n",
    "            expanded_coords = self.coordinate_cache[cache_key]\n",
    "        else: \n",
    "            coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1) \n",
    "            expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1) \n",
    "            self.coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "        x_with_coords = torch.cat([x, expanded_coords], dim=1) \n",
    "        return x_with_coords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040d5cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test both implementations\n",
    "d_hidden = 768\n",
    "num_heads = 8\n",
    "seq_length = 197\n",
    "batch_size = 4\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_length, d_hidden)\n",
    "print(\"Input shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6117bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k shape after linear: torch.Size([4, 197, 768])\n",
      "v shape after linear: torch.Size([4, 197, 768])\n",
      "k shape after transpose: torch.Size([4, 768, 197])\n",
      "v shape after transpose: torch.Size([4, 768, 197])\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "convnn= MultiHeadConvNNAttention_NoBatchSplit(\n",
    "    d_hidden=d_hidden,\n",
    "    num_heads=8,\n",
    "    attention_dropout=0.1,\n",
    "    K=4,\n",
    "    sampling_type='all',  # 'all', 'random', 'spatial'\n",
    "    num_samples=-1, \n",
    "    sample_padding=0, \n",
    "    magnitude_type='euclidean',  # 'euclidean' or 'cosine'\n",
    "    coordinate_encoding=False\n",
    ")\n",
    "ex = convnn(x)\n",
    "print(\"Output shape:\", ex.shape)  # Expected: (B, seq_length, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b7cfe",
   "metadata": {},
   "source": [
    "## Oct 7, 2025\n",
    "### Figure out a way to add number of heads to the ConvNN Attention layer similar to the MultiHeadAttention layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d62a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526eb58",
   "metadata": {},
   "source": [
    "#### 1. MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62437a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Multi-Head Layers for Transformer Encoder\"\"\"\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_hidden, num_heads, attention_dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_hidden // num_heads # dimension of each head\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)        \n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        print()\n",
    "        print(\"[Inside scaled_dot_product_attention]\")\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        print(\"attn_scores shape:\", attn_scores.shape)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))\n",
    "        print(\"attn_probs shape:\", attn_probs.shape)\n",
    "        print(\"V shape:\", V.shape)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        print(\"output shape:\", output.shape)\n",
    "        return output, attn_probs\n",
    "    \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"[Inside MultiHeadAttention forward]\")\n",
    "        q = self.split_head(self.W_q(x)) # (B, num_heads, seq_length, d_k)\n",
    "        k = self.split_head(self.W_k(x))\n",
    "        v = self.split_head(self.W_v(x))\n",
    "        print(\"original x shape:\", x.shape)\n",
    "        print(\"kqv shape:\", k.shape, q.shape, v.shape)\n",
    "        \n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask) # (B, num_heads, seq_length, d_k)\n",
    "        output = self.W_o(self.combine_heads(attn_output)) # (B, seq_length, d_hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf2165fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n",
      "--------------------------------------------------\n",
      "[Inside MultiHeadAttention forward]\n",
      "original x shape: torch.Size([4, 197, 768])\n",
      "kqv shape: torch.Size([4, 3, 197, 256]) torch.Size([4, 3, 197, 256]) torch.Size([4, 3, 197, 256])\n",
      "\n",
      "[Inside scaled_dot_product_attention]\n",
      "attn_scores shape: torch.Size([4, 3, 197, 197])\n",
      "attn_probs shape: torch.Size([4, 3, 197, 197])\n",
      "V shape: torch.Size([4, 3, 197, 256])\n",
      "output shape: torch.Size([4, 3, 197, 256])\n",
      "--------------------------------------------------\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test both implementations\n",
    "d_hidden = 768\n",
    "num_heads = 3\n",
    "seq_length = 197\n",
    "batch_size = 4\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_length, d_hidden)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "attention = MultiHeadAttention(d_hidden=d_hidden, num_heads=num_heads, attention_dropout=0.1)\n",
    "out = attention(x)\n",
    "print(\"-\"*50)\n",
    "print(\"Output shape:\", out.shape)  # Expected: (B, seq_length, d_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d795c",
   "metadata": {},
   "source": [
    "#### 2. Original ConvNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e079b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working implementation \n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else (d_hidden // num_heads)\n",
    "        self.out_channels = (d_hidden // num_heads) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1.1\n",
    "        self.NEG_INF = -0.1 \n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.batch_combine(self.split_head(self.W_k(x)))\n",
    "        v = self.batch_combine(self.split_head(self.W_v(x)))\n",
    "        \n",
    "        # k = self.batch_combine(self.split_head(x))\n",
    "        # v = self.batch_combine(self.split_head(x))\n",
    "\n",
    "        # 2. Add Coordinate Encoding \n",
    "        k = self._add_coordinate_encoding(k) if self.coordinate_encoding else k\n",
    "        v = self._add_coordinate_encoding(v) if self.coordinate_encoding else v\n",
    "\n",
    "\n",
    "        # 3. Sampling & Similarity Calculation\n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "            # q = self.batch_combine(self.split_head(x))\n",
    "            \n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix(k, q, sqrt=True)\n",
    "\n",
    "            # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "            \n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "            # prime = self._prime_temperature(v, similarity_matrix, self.K, self.maximum, temperature=1) ## New Prime with Temperature Scaling\n",
    "\n",
    "        elif self.sampling_type == 'random': # Random Samples\n",
    "            rand_idx = torch.randperm(x.shape[1], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, rand_idx, :]            \n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(rand_idx), device=q.device)\n",
    "            similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "            # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'spatial': # Spatial Samples\n",
    "            spat_idx = torch.linspace(0 + self.sample_padding, x.shape[1] - self.sample_padding - 1, self.num_samples, device=x.device).long()\n",
    "            x_sample = x[:, spat_idx, :]\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(spat_idx), device=q.device)\n",
    "            similarity_matrix[:, spat_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "            # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, spat_idx, self.maximum)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial']\")\n",
    "\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        # 6. Final Linear Projection\n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x)))\n",
    "        return x       \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1) \n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime \n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "\n",
    "    def _prime_temperature(self, v, qk, K, maximum, temperature=1.0):\n",
    "        b, c, t = v.shape\n",
    "\n",
    "        # Get top-k values and indices\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "\n",
    "        # Normalize the top-k values to create attention weights\n",
    "        if maximum:  # Cosine similarity\n",
    "            topk_weights = F.softmax(topk_values / temperature, dim=-1)\n",
    "        else:  # Euclidean distance\n",
    "            topk_weights = F.softmax(-topk_values / temperature, dim=-1)\n",
    "\n",
    "        # Expand for gathering\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_weights_exp = topk_weights.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Gather and weight\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K)\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = prime * topk_weights_exp  # Now using normalized weights\n",
    "\n",
    "        return prime.view(b, c, -1)\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indicies back to original matrix positions \n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=-1)\n",
    "        topk_indices_exp = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Expand topk values to match the shape of indices\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        ones = torch.ones((b, c, t, 1), device=v.device)\n",
    "        topk_values_exp = torch.cat((ones, topk_values_exp), dim=-1)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting \n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()    \n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, c, t = x.shape \n",
    "        cache_key = f\"{b}_{t}_{x.device}\"\n",
    "        if cache_key in self.coordinate_cache: \n",
    "            expanded_coords = self.coordinate_cache[cache_key]\n",
    "        else: \n",
    "            coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1) \n",
    "            expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1) \n",
    "            self.coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "        x_with_coords = torch.cat([x, expanded_coords], dim=1) \n",
    "        return x_with_coords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e00ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test both implementations\n",
    "d_hidden = 768\n",
    "num_heads = 3\n",
    "seq_length = 197\n",
    "batch_size = 4\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_length, d_hidden)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "convnn_params = {\n",
    "    \"K\": 9,\n",
    "    \"sampling_type\": 'all',  # 'all', 'random', 'spatial'\n",
    "    \"num_samples\": -1, \n",
    "    \"sample_padding\": 0, \n",
    "    \"magnitude_type\": 'cosine',  # 'euclidean' or 'cosine'\n",
    "    \"coordinate_encoding\": False\n",
    "}\n",
    "\n",
    "ConvNN = MultiHeadConvNNAttention(d_hidden=d_hidden, num_heads=num_heads, attention_dropout=0.1, **convnn_params)\n",
    "out = ConvNN(x)\n",
    "print(\"-\"*50)\n",
    "print(\"Output shape:\", out.shape)  # Expected: (B, seq_length, d_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381a062",
   "metadata": {},
   "source": [
    "#### Modified ConvNN Attention with change to number of heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9de8f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working implementation \n",
    "class MultiHeadConvNNAttention_Modified(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention_Modified, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "\n",
    "        # Change out_features of V projection \n",
    "        self.v_out_features = self.d_k // self.K\n",
    "        print(self.v_out_features)\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, self.v_out_features)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        # self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else (d_hidden // num_heads)\n",
    "        self.in_channels = (self.v_out_features) + 1 if coordinate_encoding else (self.v_out_features)\n",
    "        self.out_channels = (d_hidden) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1.1\n",
    "        self.NEG_INF = -0.1 \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        print(\"k shape after W_k:\", k.shape)\n",
    "        print(\"v shape after W_v:\", v.shape)\n",
    "\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        print(\"k shape after transpose:\", k.shape)\n",
    "        print(\"v shape after transpose:\", v.shape)\n",
    "\n",
    "\n",
    "        # 2. Add Coordinate Encoding \n",
    "        k = self._add_coordinate_encoding(k) if self.coordinate_encoding else k\n",
    "        v = self._add_coordinate_encoding(v) if self.coordinate_encoding else v\n",
    "\n",
    "\n",
    "        # 3. Sampling & Similarity Calculation\n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            q = self.W_q(x)\n",
    "            print(\"q shape after W_q:\", q.shape)\n",
    "            q = q.transpose(1, 2)\n",
    "            print(\"q shape after transpose:\", q.shape)\n",
    "\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix(k, q, sqrt=True)\n",
    "            print()\n",
    "            print(\"similarity_matrix shape:\", similarity_matrix.shape)\n",
    "\n",
    "            # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "            \n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "            print(\"prime shape after _prime:\", prime.shape)\n",
    "            # prime = self._prime_temperature(v, similarity_matrix, self.K, self.maximum, temperature=1) ## New Prime with Temperature Scaling\n",
    "\n",
    "        elif self.sampling_type == 'random': # Random Samples\n",
    "            rand_idx = torch.randperm(x.shape[1], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, rand_idx, :]            \n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(rand_idx), device=q.device)\n",
    "            similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "            # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'spatial': # Spatial Samples\n",
    "            spat_idx = torch.linspace(0 + self.sample_padding, x.shape[1] - self.sample_padding - 1, self.num_samples, device=x.device).long()\n",
    "            x_sample = x[:, spat_idx, :]\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(spat_idx), device=q.device)\n",
    "            similarity_matrix[:, spat_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "            # similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, spat_idx, self.maximum)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial']\")\n",
    "        print()\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "        print(\"x shape after conv:\", x.shape)\n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        print(\"x shape after permute:\", x.shape)\n",
    "\n",
    "        # 6. Final Linear Projection\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        print(\"x shape after W_o:\", x.shape)\n",
    "        print()\n",
    "        return x       \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1) \n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        print(\"[Inside _prime]\")\n",
    "        b, c, t = v.shape\n",
    "    \n",
    "        print(\"v shape:\", v.shape)\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        print(\"topk_values shape:\", topk_values.shape)\n",
    "        print(\"topk_indices shape:\", topk_indices.shape)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        print(\"topk_indices_exp shape:\", topk_indices_exp.shape)\n",
    "        print(\"topk_values_exp shape:\", topk_values_exp.shape)\n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        print(\"v_expanded shape:\", v_expanded.shape)\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        print(\"prime shape after gather:\", prime.shape)\n",
    "        prime = topk_values_exp * prime \n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "\n",
    "    def _prime_temperature(self, v, qk, K, maximum, temperature=1.0):\n",
    "        b, c, t = v.shape\n",
    "\n",
    "        # Get top-k values and indices\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "\n",
    "        # Normalize the top-k values to create attention weights\n",
    "        if maximum:  # Cosine similarity\n",
    "            topk_weights = F.softmax(topk_values / temperature, dim=-1)\n",
    "        else:  # Euclidean distance\n",
    "            topk_weights = F.softmax(-topk_values / temperature, dim=-1)\n",
    "\n",
    "        # Expand for gathering\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_weights_exp = topk_weights.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Gather and weight\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K)\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = prime * topk_weights_exp  # Now using normalized weights\n",
    "\n",
    "        return prime.view(b, c, -1)\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indicies back to original matrix positions \n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=-1)\n",
    "        topk_indices_exp = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Expand topk values to match the shape of indices\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        ones = torch.ones((b, c, t, 1), device=v.device)\n",
    "        topk_values_exp = torch.cat((ones, topk_values_exp), dim=-1)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting \n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()    \n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, c, t = x.shape \n",
    "        cache_key = f\"{b}_{t}_{x.device}\"\n",
    "        if cache_key in self.coordinate_cache: \n",
    "            expanded_coords = self.coordinate_cache[cache_key]\n",
    "        else: \n",
    "            coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1) \n",
    "            expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1) \n",
    "            self.coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "        x_with_coords = torch.cat([x, expanded_coords], dim=1) \n",
    "        return x_with_coords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84df62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 197, 768])\n",
      "--------------------------------------------------\n",
      "28\n",
      "k shape after W_k: torch.Size([4, 197, 768])\n",
      "v shape after W_v: torch.Size([4, 197, 28])\n",
      "k shape after transpose: torch.Size([4, 768, 197])\n",
      "v shape after transpose: torch.Size([4, 28, 197])\n",
      "q shape after W_q: torch.Size([4, 197, 768])\n",
      "q shape after transpose: torch.Size([4, 768, 197])\n",
      "\n",
      "similarity_matrix shape: torch.Size([4, 197, 197])\n",
      "[Inside _prime]\n",
      "v shape: torch.Size([4, 28, 197])\n",
      "topk_values shape: torch.Size([4, 197, 9])\n",
      "topk_indices shape: torch.Size([4, 197, 9])\n",
      "topk_indices_exp shape: torch.Size([4, 28, 197, 9])\n",
      "topk_values_exp shape: torch.Size([4, 28, 197, 9])\n",
      "v_expanded shape: torch.Size([4, 28, 197, 9])\n",
      "prime shape after gather: torch.Size([4, 28, 197, 9])\n",
      "prime shape after _prime: torch.Size([4, 28, 1773])\n",
      "\n",
      "x shape after conv: torch.Size([4, 768, 197])\n",
      "x shape after permute: torch.Size([4, 197, 768])\n",
      "x shape after W_o: torch.Size([4, 197, 768])\n",
      "\n",
      "--------------------------------------------------\n",
      "Output shape: torch.Size([4, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test both implementations\n",
    "d_hidden = 768\n",
    "num_heads = 3\n",
    "seq_length = 197\n",
    "batch_size = 4\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_length, d_hidden)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "convnn_params = {\n",
    "    \"K\": 9,\n",
    "    \"sampling_type\": 'all',  # 'all', 'random', 'spatial'\n",
    "    \"num_samples\": -1, \n",
    "    \"sample_padding\": 0, \n",
    "    \"magnitude_type\": 'cosine',  # 'euclidean' or 'cosine'\n",
    "    \"coordinate_encoding\": False\n",
    "}\n",
    "\n",
    "ConvNN = MultiHeadConvNNAttention_Modified(d_hidden=d_hidden, num_heads=num_heads, attention_dropout=0.1, **convnn_params)\n",
    "out = ConvNN(x)\n",
    "print(\"-\"*50)\n",
    "print(\"Output shape:\", out.shape)  # Expected: (B, seq_length, d_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb63c00",
   "metadata": {},
   "source": [
    "# Oct 11 Make ConvNN Attention same as MultiheadAttention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c92f28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc848ae",
   "metadata": {},
   "source": [
    "### i. MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbb98f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Multi-Head Layers for Transformer Encoder\"\"\"\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_hidden, num_heads, attention_dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_hidden // num_heads # dimension of each head\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        print(\"\\n[Scaled Dot Product Attention]\")\n",
    "        print(\"attn_scores shape:\", attn_scores.shape)\n",
    "        print(\"attn_scores:\", attn_scores)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))\n",
    "        print(\"attn_probs shape:\", attn_probs.shape)\n",
    "        print(\"attn_probs:\", attn_probs)\n",
    "        print(\"v shape: \", V.shape)\n",
    "        print(\"v: \", V)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        print(\"output shape:\", output.shape)\n",
    "        print(\"output:\", output)\n",
    "        return output, attn_probs\n",
    "    \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        q = self.split_head(self.W_q(x)) # (B, num_heads, seq_length, d_k)\n",
    "        k = self.split_head(self.W_k(x))\n",
    "        v = self.split_head(self.W_v(x))\n",
    "        \n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask) # (B, num_heads, seq_length, d_k)\n",
    "        output = self.W_o(self.combine_heads(attn_output)) # (B, seq_length, d_hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a812c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12., 13., 14., 15., 16.],\n",
      "         [17., 18., 19., 20., 21., 22., 23., 24.],\n",
      "         [25., 26., 27., 28., 29., 30., 31., 32.],\n",
      "         [33., 34., 35., 36., 37., 38., 39., 40.],\n",
      "         [41., 42., 43., 44., 45., 46., 47., 48.]]])\n",
      "torch.Size([1, 6, 8])\n",
      "\n",
      "[Scaled Dot Product Attention]\n",
      "attn_scores shape: torch.Size([1, 1, 6, 6])\n",
      "attn_scores: tensor([[[[  3665.6416,  10182.3379,  16699.0332,  23215.7305,  29732.4258,\n",
      "            36249.1211],\n",
      "          [ 10182.3379,  28284.2715,  46386.2070,  64488.1406,  82590.0703,\n",
      "           100692.0078],\n",
      "          [ 16699.0332,  46386.2070,  76073.3750, 105760.5469, 135447.7188,\n",
      "           165134.8906],\n",
      "          [ 23215.7305,  64488.1406, 105760.5469, 147032.9531, 188305.3750,\n",
      "           229577.7812],\n",
      "          [ 29732.4258,  82590.0703, 135447.7188, 188305.3750, 241163.0156,\n",
      "           294020.6562],\n",
      "          [ 36249.1211, 100692.0078, 165134.8906, 229577.7812, 294020.6562,\n",
      "           358463.5312]]]], grad_fn=<DivBackward0>)\n",
      "attn_probs shape: torch.Size([1, 1, 6, 6])\n",
      "attn_probs: tensor([[[[0., 0., 0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0., 0., 1.]]]], grad_fn=<SoftmaxBackward0>)\n",
      "v shape:  torch.Size([1, 1, 6, 8])\n",
      "v:  tensor([[[[ 36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.],\n",
      "          [100., 100., 100., 100., 100., 100., 100., 100.],\n",
      "          [164., 164., 164., 164., 164., 164., 164., 164.],\n",
      "          [228., 228., 228., 228., 228., 228., 228., 228.],\n",
      "          [292., 292., 292., 292., 292., 292., 292., 292.],\n",
      "          [356., 356., 356., 356., 356., 356., 356., 356.]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "output shape: torch.Size([1, 1, 6, 8])\n",
      "output: tensor([[[[356., 356., 356., 356., 356., 356., 356., 356.],\n",
      "          [356., 356., 356., 356., 356., 356., 356., 356.],\n",
      "          [356., 356., 356., 356., 356., 356., 356., 356.],\n",
      "          [356., 356., 356., 356., 356., 356., 356., 356.],\n",
      "          [356., 356., 356., 356., 356., 356., 356., 356.],\n",
      "          [356., 356., 356., 356., 356., 356., 356., 356.]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 8 \n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 1\n",
    "dropout = 0.0 \n",
    "\n",
    "x = torch.Tensor(\n",
    "    [\n",
    "        [\n",
    "            [ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "            [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "            [17, 18, 19, 20, 21, 22, 23, 24],\n",
    "            [25, 26, 27, 28, 29, 30, 31, 32],\n",
    "            [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "            [41, 42, 43, 44, 45, 46, 47, 48],\n",
    "        ]\n",
    "    ]\n",
    ").to(torch.float32)\n",
    "\n",
    "# x = torch.randint(0, 10, (batch_size, seq_length, d_hidden)).to(torch.float32)\n",
    "# x = torch.randn(batch_size, seq_length, d_hidden).to(torch.float32)\n",
    "print(x)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "attention = MultiHeadAttention(d_hidden=d_hidden, num_heads=num_heads, attention_dropout=dropout)\n",
    "attention.W_k.weight.data.fill_(1.0)\n",
    "attention.W_q.weight.data.fill_(1.0)\n",
    "attention.W_v.weight.data.fill_(1.0)\n",
    "attention.W_o.weight.data.fill_(1.0)\n",
    "out = attention(x)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfb1d3",
   "metadata": {},
   "source": [
    "### ii. ConvNN Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e801958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else (d_hidden // num_heads)\n",
    "        self.out_channels = (d_hidden // num_heads) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "            bias = False, \n",
    "            groups=self.in_channels\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1.1\n",
    "        self.NEG_INF = -0.1 \n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.batch_combine(self.split_head(self.W_k(x)))\n",
    "        v = self.batch_combine(self.split_head(self.W_v(x)))\n",
    "        # v = self.batch_combine(self.split_head(x))\n",
    "        \n",
    "        # 3. Sampling & Similarity Calculation\n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "\n",
    "            similarity_matrix = self._calculate_attention_matrix(k, q)\n",
    "\n",
    "            similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "            \n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        print(\"\\n[After Convolution]\")\n",
    "        print(\"output size: \", x.shape) \n",
    "        print(x)\n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        # 6. Final Linear Projection\n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x)))\n",
    "        return x       \n",
    "    def _calculate_attention_matrix(self, K, Q):\n",
    "        attn_score = torch.matmul(K.transpose(1, 2), Q) / np.sqrt(self.d_k)\n",
    "        return attn_score\n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1) \n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        print(\"Before _prime:\")\n",
    "        print(\"v shape:\", v.shape)\n",
    "        print(\"similarity_matrix shape:\", qk.shape)\n",
    "        print(\"similarity_matrix:\", qk)\n",
    "        b, c, t = v.shape\n",
    "        \n",
    "        print(\"Inside Prime\")   \n",
    "        print(\"v shape:\", v.shape)\n",
    "        print(\"K:\", K)\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        print(\"topk_indices shape:\", topk_indices.shape)\n",
    "        print(\"topk_indices:\", topk_indices, \"\\n\")\n",
    "        print(\"topk_values shape:\", topk_values.shape)\n",
    "        print(\"topk_values:\", topk_values)\n",
    "\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        topk_values_exp = torch.softmax(topk_values_exp, dim=-1)\n",
    "        print(\"topk_values_exp shape:\", topk_values_exp.shape)\n",
    "        print(\"topk_values_exp:\", topk_values_exp)\n",
    "\n",
    "        # #### SOFTMAX ON TOP-K VALUES ####\n",
    "        # topk_values_exp = torch.softmax(topk_values_exp, dim=-1)        \n",
    "        # # print(topk_values_exp.shape, topk_indices_exp.shape)\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"v unsqueeze shape: \", v.unsqueeze(1).shape)\n",
    "        print(\"v unsqueeze(1): \", v.unsqueeze(1))\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        print(\"prime shape after gather:\", prime.shape)\n",
    "        print(\"prime after gather:\", prime)\n",
    "        print(\"topk_values_exp shape:\", topk_values_exp.shape)\n",
    "        print(\"topk_values_exp:\", topk_values_exp)\n",
    "        prime = topk_values_exp * prime \n",
    "        \n",
    "        print(\"prime shape after weighting:\", prime.shape)\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        print(\"prime shape after view:\", prime.shape)\n",
    "        print(\"prime after view:\", prime)\n",
    "\n",
    "        return prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d0f2815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 8])\n",
      "Before _prime:\n",
      "v shape: torch.Size([1, 8, 6])\n",
      "similarity_matrix shape: torch.Size([1, 6, 6])\n",
      "similarity_matrix: tensor([[[0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.]]], grad_fn=<SoftmaxBackward0>)\n",
      "Inside Prime\n",
      "v shape: torch.Size([1, 8, 6])\n",
      "K: 6\n",
      "topk_indices shape: torch.Size([1, 6, 6])\n",
      "topk_indices: tensor([[[5, 1, 2, 3, 4, 0],\n",
      "         [5, 1, 2, 3, 4, 0],\n",
      "         [5, 1, 2, 3, 4, 0],\n",
      "         [5, 1, 2, 3, 4, 0],\n",
      "         [5, 1, 2, 3, 4, 0],\n",
      "         [5, 1, 2, 3, 4, 0]]]) \n",
      "\n",
      "topk_values shape: torch.Size([1, 6, 6])\n",
      "topk_values: tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.]]], grad_fn=<TopkBackward0>)\n",
      "topk_values_exp shape: torch.Size([1, 8, 6, 6])\n",
      "topk_values_exp: tensor([[[[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "v unsqueeze shape:  torch.Size([1, 1, 8, 6])\n",
      "v unsqueeze(1):  tensor([[[[ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.],\n",
      "          [ 36., 100., 164., 228., 292., 356.]]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "prime shape after gather: torch.Size([1, 8, 6, 6])\n",
      "prime after gather: tensor([[[[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]],\n",
      "\n",
      "         [[356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.],\n",
      "          [356., 100., 164., 228., 292.,  36.]]]], grad_fn=<GatherBackward0>)\n",
      "topk_values_exp shape: torch.Size([1, 8, 6, 6])\n",
      "topk_values_exp: tensor([[[[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]],\n",
      "\n",
      "         [[0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296],\n",
      "          [0.3522, 0.1296, 0.1296, 0.1296, 0.1296, 0.1296]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "prime shape after weighting: torch.Size([1, 8, 6, 6])\n",
      "prime shape after view: torch.Size([1, 8, 36])\n",
      "prime after view: tensor([[[125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643],\n",
      "         [125.3787,  12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,\n",
      "           12.9563,  21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,\n",
      "           21.2483,  29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,\n",
      "           29.5403,  37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,\n",
      "           37.8323,   4.6643, 125.3787,  12.9563,  21.2483,  29.5403,  37.8323,\n",
      "            4.6643]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "[After Convolution]\n",
      "output size:  torch.Size([1, 8, 6])\n",
      "tensor([[[231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200],\n",
      "         [231.6200, 231.6200, 231.6200, 231.6200, 231.6200, 231.6200]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 8 \n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 1\n",
    "dropout = 0.0 \n",
    "\n",
    "# x = torch.Tensor(\n",
    "#     [\n",
    "#         [\n",
    "#             [ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "#             [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "#             [17, 18, 19, 20, 21, 22, 23, 24],\n",
    "#             [25, 26, 27, 28, 29, 30, 31, 32],\n",
    "#             [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "#             [41, 42, 43, 44, 45, 46, 47, 48],\n",
    "#         ]\n",
    "#     ]\n",
    "# ).to(torch.float32)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "convnn = MultiHeadConvNNAttention(d_hidden=d_hidden, \n",
    "                                  num_heads=num_heads, \n",
    "                                  attention_dropout=dropout, \n",
    "                                  K=6, \n",
    "                                  sampling_type='all',\n",
    "                                  num_samples=-1,\n",
    "                                  sample_padding=0,\n",
    "                                  magnitude_type='cosine',\n",
    "                                  coordinate_encoding=False,\n",
    "                                  seq_length=6\n",
    "                                  )\n",
    "\n",
    "convnn.W_k.weight.data.fill_(1.0)\n",
    "convnn.W_q.weight.data.fill_(1.0)\n",
    "convnn.W_v.weight.data.fill_(1.0)\n",
    "convnn.W_o.weight.data.fill_(1.0)\n",
    "convnn.conv.weight.data.fill_(1.0)\n",
    "out = convnn(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcbcb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# difference: convnn out - attention = 4.1027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef1177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee41c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
