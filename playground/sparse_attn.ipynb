{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a1da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6472dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "import numpy as np\n",
    "import math  \n",
    "\n",
    "\n",
    "\n",
    "### Code from : https://github.com/openai/sparse_attention/blob/master/utils.py ###\n",
    "def get_attn_mask(n, attn_mode, local_attn_ctx=None, device='cuda'):\n",
    "    if attn_mode == 'all':\n",
    "        # ✓ BIDIRECTIONAL - all patches attend to all patches\n",
    "        b = torch.ones(n, n, device=device)\n",
    "    \n",
    "    elif attn_mode == 'local':\n",
    "        # ✓ BIDIRECTIONAL LOCAL - attend to nearby patches in both directions\n",
    "        bandwidth = local_attn_ctx\n",
    "        # Create a band matrix (not just lower triangular)\n",
    "        b = torch.zeros(n, n, device=device)\n",
    "        for i in range(n):\n",
    "            start = max(0, i - bandwidth // 2)\n",
    "            end = min(n, i + bandwidth // 2 + 1)\n",
    "            b[i, start:end] = 1\n",
    "    \n",
    "    elif attn_mode == 'strided':\n",
    "        # ✓ BIDIRECTIONAL STRIDED\n",
    "        stride = local_attn_ctx\n",
    "        x = torch.arange(n, dtype=torch.int32, device=device).view(n, 1)\n",
    "        y = x.t()\n",
    "        q = x.expand(n, n)\n",
    "        k = y.expand(n, n)\n",
    "        # Remove c1 = q >= k (this was the causal constraint!)\n",
    "        c2 = ((q - k).abs() % stride) == 0  # Distance is multiple of stride\n",
    "        b = c2.float()\n",
    "    \n",
    "    b = b.view(1, 1, n, n)\n",
    "    return b\n",
    "\n",
    "\n",
    "def strided_transpose(x, n_ctx, local_attn_ctx, blocksize=None):\n",
    "    \"\"\"\n",
    "    Transpose for strided attention pattern.\n",
    "    \n",
    "    Args:\n",
    "        x: tensor of shape [batch, seq_len, embd]\n",
    "        n_ctx: context length\n",
    "        local_attn_ctx: stride length\n",
    "        blocksize: not used in PyTorch version (kept for API compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        transposed tensor\n",
    "    \"\"\"\n",
    "    bT_ctx = n_ctx // local_attn_ctx\n",
    "    n, t, embd = x.shape\n",
    "    x = x.view(n, bT_ctx, local_attn_ctx, embd)\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    x = x.reshape(n, t, embd)\n",
    "    return x\n",
    "\n",
    "\n",
    "def split_heads(x, n_heads):\n",
    "    \"\"\"\n",
    "    Split the last dimension into (n_heads, depth).\n",
    "    Transpose to shape [batch, n_heads, seq_len, depth]\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    depth = d_model // n_heads\n",
    "    x = x.view(batch_size, seq_len, n_heads, depth)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "def merge_heads(x):\n",
    "    \"\"\"\n",
    "    Merge heads back to original shape.\n",
    "    Input: [batch, n_heads, seq_len, depth]\n",
    "    Output: [batch, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    batch_size, n_heads, seq_len, depth = x.shape\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    return x.reshape(batch_size, seq_len, n_heads * depth)\n",
    "\n",
    "\n",
    "def attention_impl(q, k, v, n_heads, attention_dropout, attn_mode, local_attn_ctx=None):\n",
    "    \"\"\"\n",
    "    Standard attention implementation with different masking patterns.\n",
    "    \n",
    "    Args:\n",
    "        q, k, v: query, key, value tensors of shape [batch, seq_len, d_model]\n",
    "        n_heads: number of attention heads\n",
    "        attn_mode: attention pattern ('all', 'local', 'strided')\n",
    "        local_attn_ctx: context window for local/strided attention\n",
    "    \n",
    "    Returns:\n",
    "        attention output of shape [batch, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # Split heads: [batch, n_heads, seq_len, depth]\n",
    "    q = split_heads(q, n_heads)\n",
    "    k = split_heads(k, n_heads)\n",
    "    v = split_heads(v, n_heads)\n",
    "    \n",
    "    # Get attention mask\n",
    "    n_timesteps = k.shape[2]\n",
    "    mask = get_attn_mask(n_timesteps, attn_mode, local_attn_ctx, device=q.device)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    # [batch, n_heads, seq_len, seq_len]\n",
    "    depth = q.shape[-1]\n",
    "    scale_amount = 1.0 / np.sqrt(depth)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    w = torch.matmul(q, k.transpose(-2, -1))\n",
    "    w = w * scale_amount\n",
    "    \n",
    "    # Apply mask (using large negative value for masked positions)\n",
    "    w = w * mask + -1e9 * (1 - mask)\n",
    "    \n",
    "    # Softmax\n",
    "    w = F.softmax(w, dim=-1)\n",
    "\n",
    "    w = F.dropout(w, p=attention_dropout)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    a = torch.matmul(w, v)\n",
    "    \n",
    "    # Merge heads\n",
    "    a = merge_heads(a)\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "class MultiHeadSparseAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head sparse attention module.\n",
    "    \n",
    "    Supports different attention patterns: 'all', 'local', 'strided'\n",
    "    \"\"\"\n",
    "    def __init__(self, d_hidden, num_heads, attention_dropout, attn_mode='all', local_attn_ctx=None):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.attn_mode = attn_mode\n",
    "        self.local_attn_ctx = local_attn_ctx\n",
    "        \n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.k_proj = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.v_proj = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "        self.out_proj = nn.Linear(d_hidden, d_hidden, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape [batch, seq_len, d_model]\n",
    "        \n",
    "        Returns:\n",
    "            output tensor of shape [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output = attention_impl(\n",
    "            q, k, v, \n",
    "            self.num_heads, \n",
    "            self.attention_dropout,\n",
    "            self.attn_mode, \n",
    "            self.local_attn_ctx\n",
    "        )\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# For gradient checkpointing (equivalent to @recomputable decorator)\n",
    "def checkpoint_attention(q, k, v, n_heads, attn_mode, local_attn_ctx=None):\n",
    "    \"\"\"\n",
    "    Attention with gradient checkpointing to save memory.\n",
    "    \"\"\"\n",
    "    return torch.utils.checkpoint.checkpoint(\n",
    "        attention_impl,\n",
    "        q, k, v, n_heads, attn_mode, local_attn_ctx,\n",
    "        use_reentrant=False\n",
    "    )\n",
    "\n",
    "\n",
    "def strided_attention_impl(q, k, v, n_heads, local_attn_ctx, blocksize=32):\n",
    "    \"\"\"\n",
    "    Strided attention with transposition (as in blocksparse version).\n",
    "    \n",
    "    Note: This is the dense implementation. For true block-sparse computation,\n",
    "    you would need a custom CUDA kernel or library like Triton.\n",
    "    \"\"\"\n",
    "    n_ctx = q.shape[1]\n",
    "    \n",
    "    # Apply strided transpose\n",
    "    q = strided_transpose(q, n_ctx, local_attn_ctx, blocksize)\n",
    "    k = strided_transpose(k, n_ctx, local_attn_ctx, blocksize)\n",
    "    v = strided_transpose(v, n_ctx, local_attn_ctx, blocksize)\n",
    "    \n",
    "    # Apply attention\n",
    "    a = attention_impl(q, k, v, n_heads, 'strided', local_attn_ctx)\n",
    "    \n",
    "    # Reverse the transpose\n",
    "    n, t, embd = a.shape\n",
    "    bT_ctx = n_ctx // local_attn_ctx\n",
    "    a = a.view(n, local_attn_ctx, bT_ctx, embd)\n",
    "    a = a.permute(0, 2, 1, 3)\n",
    "    a = a.reshape(n, t, embd)\n",
    "    \n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f769e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
