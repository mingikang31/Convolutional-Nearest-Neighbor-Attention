Differentiate Experiments for ConvNN Attention ViT

1. ConvNN Attention KQV Projection 
2. ConvNN Attention V Projection 
3. Branching (Conv1d + ConvNN Attention) & (Attention + ConvNN Attention) 
4. ConvNN Attention KQV Projection + softmax on similarity matrix 
5. ConvNN Attention KQV Projection + No Dropout after prime convolution 


Things to try out later??
6. softmax with temperature inside prime function 

    def _prime(self, v, qk, K, maximum, temperature=1.0):
        b, c, t = v.shape
        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)

        # Normalize the topk-values to create attention weights 
        if maximum: # Cosine
            topk_weights = F.softmax(topk_values / temperature, dim=-1) 
        else: # Euclidean
            topk_weights = F.softmax(-topk_values / temperature, dim=-1)




        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)
        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)

        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()
        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)
        prime = topk_values_exp * prime 

        prime = prime.view(b, c, -1)

        return prime
- This does not work on ConvNN Attention alone ~59% but for branching it does better ~64.5%

7. Normalize topk_values after selecting topk indices so that they sum up to 1. 
- In attention, row probability sum up to 1 when softmax, but when ConvNN selects K neighbors, K similarities may not add up to 1
- Normalize over K probabilities 

8. Softmax topk_values after selecting topk_indices based on topk_values 
    b, c, t = v.shape
    topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)
    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)
    topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)

    # #### SOFTMAX ON TOP-K VALUES ####
    topk_values_exp = torch.softmax(topk_values_exp, dim=-1)        
    print(topk_values_exp.shape, topk_indices_exp.shape)

- ~3% decrease in accuracy when softmax on the topk_values 

ConvNNAttention_All_K9_s42_NH1_topksoftmax

9. Have num_heads = 1
- Parameters for ConvNN Attention increase drastically, beats Attention at K=36 by 0.5%. 

ConvNNAttention_All_K9_s42_NH1

10. Change V_out_features = self.d_hidden // (self.num_heads**2) 
- having less channels for v helps with parameters for v's linear layer and Conv1d (in_channels, out_channels) 
    - Where in_channels = self.d_hidden // (self.num_heads**2) and out_channels = d_hidden

ConvNNAttention_All_K9_s42_NH3_NoSplitHead


11. Change V_out_features = self.num_heads 
- Changing so that the parameters are less when K's are higher 
- having less channels for v helps with parameters for v's linear layer and Conv1d (in_channels, out_channels) 
    - Where in_channels = self.num_heads and out_channels = d_hidden

12. Change convolution for prime to depthwise separable convolution to reduce number of parameters. 
- ConvNNAttention with num_heads = 1:
    - K = 4  ->  7,260,298
    - K = 9  ->  9,472,138
    - K = 16 -> 12,568,714
    - K = 25 -> 16,550,062
    - K = 36 -> 21,416,074
    - K = 49 -> 27,166,858
- ConvNNAttention_Depthwise: 
    - K = 4  ->  5,944,714
    - K = 9  ->  5,956,234
    - K = 16 ->  5,972,362
    - K = 25 ->  5,993,098
    - K = 36 ->  6,018,442
    - K = 49 ->  6,048,394

- depthwise reduces parameter size by a big factor, however it does not reach above 61% no matter the K value (speed is just as slow as doing regular convolution) 


13. KQ Projections vs. KQV Projections 
- KQ # params < KQV # params (~400,000 difference)
- KQ Accuracy < KQV Accuracy (~0.5-1.0% difference)

14. 




*** Best Results & Notes ***
- V projection with dropout is 65%, 
- when num_heads = 1 and K > 36, ConvNN-Attention beats attention (Original MultiHeadConvNNAttention)

- Speed and parameter decrease when doing 10 + 11: lweroing v's linear projection channels making conv1d's parameters less. Decreases performance a bit ~2-4%.




